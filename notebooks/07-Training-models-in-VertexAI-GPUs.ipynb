{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67c8abc",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Training Models in Vertex AI: PyTorch Example\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- When should you consider a GPU (or TPU) instance for PyTorch training in Vertex AI, and what are the trade‑offs for small vs. large workloads?\n",
    "- How do you launch a script‑based training job and write **all** artifacts (model, metrics, logs) next to each other in GCS without deploying a managed model?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Prepare the Titanic dataset and save train/val arrays to compressed `.npz` files in GCS.\n",
    "- Submit a **CustomTrainingJob** that runs a PyTorch script and explicitly writes outputs to a chosen `gs://…/artifacts/.../` folder.\n",
    "- Co‑locate artifacts: `model.pt` (or `.joblib`), `metrics.json`, `eval_history.csv`, and `training.log` for reproducibility.\n",
    "- Choose CPU vs. GPU instances sensibly; understand when distributed training is (not) worth it.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Initial setup (controller notebook)\n",
    "\n",
    "Open a fresh Jupyter notebook in Vertex AI Workbench (Instances tab) and initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, storage\n",
    "import datetime as dt\n",
    "\n",
    "PROJECT_ID = \"your-gcp-project-id\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"your-bucket\"  # same region as REGION\n",
    "\n",
    "# Only used for the SDK's small packaging tarball.\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b20f6",
   "metadata": {},
   "source": [
    "### Select the PyTorch environment (kernel)\n",
    "- In JupyterLab, click the kernel name (top‑right) and switch to a **PyTorch‑ready** kernel. On Workbench Instances this is usually available out‑of‑the‑box; if `import torch` fails, install locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision --upgrade\n",
    "  ```\n",
    "- Quick check that your kernel can see PyTorch (and optionally CUDA if your VM has a GPU):\n",
    "  ```python\n",
    "  import torch\n",
    "  print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
    "  ```\n",
    "- Note: local PyTorch is only needed for **local tests**. Your **Vertex AI job** uses the container specified by `container_uri` (e.g., `pytorch-cpu.2-1` or `pytorch-gpu.2-1`), so it brings its own framework at run time.\n",
    "\n",
    "Notes:\n",
    "- The staging bucket only stores the SDK’s temporary tar.gz of your training code.\n",
    "- We will **not** use `base_output_dir`; your script will write everything under a single `gs://…/artifacts/.../` path.\n",
    "\n",
    "## Prepare data as `.npz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f61265",
   "metadata": {},
   "source": [
    "python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load Titanic CSV (from local or GCS you've already downloaded to the notebook)\n",
    "df = pd.read_csv(\"titanic_train.csv\")\n",
    "\n",
    "# Minimal preprocessing to numeric arrays\n",
    "sex_enc = LabelEncoder().fit(df[\"Sex\"])  \n",
    "df[\"Sex\"] = sex_enc.transform(df[\"Sex\"])  \n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n",
    "emb_enc = LabelEncoder().fit(df[\"Embarked\"])  \n",
    "df[\"Embarked\"] = emb_enc.transform(df[\"Embarked\"])  \n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())\n",
    "\n",
    "X = df[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]].values\n",
    "y = df[\"Survived\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "np.savez(\"train_data.npz\", X_train=X_train, y_train=y_train)\n",
    "np.savez(\"val_data.npz\",   X_val=X_val,   y_val=y_val)\n",
    "\n",
    "# Upload to GCS\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "bucket.blob(\"data/train_data.npz\").upload_from_filename(\"train_data.npz\")\n",
    "bucket.blob(\"data/val_data.npz\").upload_from_filename(\"val_data.npz\")\n",
    "print(\"Uploaded: gs://%s/data/train_data.npz and val_data.npz\" % BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    ":::::::::::::::::::::::::::::::: callout\n",
    "\n",
    "#### Why `.npz`?\n",
    "- Smaller, faster I/O than CSV for arrays.\n",
    "- Natural fit for `torch.utils.data.Dataset` / `DataLoader`.\n",
    "- One file can hold multiple arrays (`X_train`, `y_train`).\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Minimal PyTorch training script (`train_nn.py`)\n",
    "\n",
    "Place this file in your repo (e.g., `GCP_helpers/train_nn.py`). It does three things:\n",
    "1) loads `.npz` from local or GCS, 2) trains a tiny MLP, 3) **writes all outputs side‑by‑side** (model + metrics + eval history + training.log) to the same `--model_out` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fb92b",
   "metadata": {},
   "source": [
    "python\n",
    "# GCP_helpers/train_nn.py\n",
    "import argparse, io, json, os, sys\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from time import time\n",
    "\n",
    "# --- small helpers for GCS/local I/O ---\n",
    "def _parent_dir(p):\n",
    "    return p.rsplit(\"/\", 1)[0] if p.startswith(\"gs://\") else (os.path.dirname(p) or \".\")\n",
    "\n",
    "def _write_bytes(path: str, data: bytes):\n",
    "    if path.startswith(\"gs://\"):\n",
    "        try:\n",
    "            import fsspec\n",
    "            with fsspec.open(path, \"wb\") as f:\n",
    "                f.write(data)\n",
    "        except Exception:\n",
    "            from google.cloud import storage\n",
    "            b, k = path[5:].split(\"/\", 1)\n",
    "            storage.Client().bucket(b).blob(k).upload_from_string(data)\n",
    "    else:\n",
    "        os.makedirs(_parent_dir(path), exist_ok=True)\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(data)\n",
    "\n",
    "def _write_text(path: str, text: str):\n",
    "    _write_bytes(path, text.encode(\"utf-8\"))\n",
    "\n",
    "# --- tiny MLP ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 1), nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class _Tee:\n",
    "    def __init__(self, *s): self.s = s\n",
    "    def write(self, d):\n",
    "        for x in self.s: x.write(d); x.flush()\n",
    "    def flush(self):\n",
    "        for x in self.s: x.flush()\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--train\", required=True)\n",
    "    ap.add_argument(\"--val\",   required=True)\n",
    "    ap.add_argument(\"--epochs\", type=int, default=100)\n",
    "    ap.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "    ap.add_argument(\"--model_out\", required=True, help=\"gs://…/artifacts/.../model.pt\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # All artifacts will sit next to model_out\n",
    "    model_path = args.model_out\n",
    "    art_dir = _parent_dir(model_path)\n",
    "\n",
    "    # capture stdout/stderr\n",
    "    buf = io.StringIO()\n",
    "    orig_out, orig_err = sys.stdout, sys.stderr\n",
    "    sys.stdout = _Tee(sys.stdout, buf)\n",
    "    sys.stderr = _Tee(sys.stderr, buf)\n",
    "    log_path = f\"{art_dir}/training.log\"\n",
    "\n",
    "    try:\n",
    "        # Load npz (supports gs:// via fsspec)\n",
    "        def _npz_load(p):\n",
    "            if p.startswith(\"gs://\"):\n",
    "                import fsspec\n",
    "                with fsspec.open(p, \"rb\") as f:\n",
    "                    by = f.read()\n",
    "                return np.load(io.BytesIO(by))\n",
    "            else:\n",
    "                return np.load(p)\n",
    "        train = _npz_load(args.train)\n",
    "        val   = _npz_load(args.val)\n",
    "        Xtr, ytr = train[\"X_train\"].astype(\"float32\"), train[\"y_train\"].astype(\"float32\")\n",
    "        Xva, yva = val[\"X_val\"].astype(\"float32\"),   val[\"y_val\"].astype(\"float32\")\n",
    "\n",
    "        Xtr_t = torch.from_numpy(Xtr).to(device)\n",
    "        ytr_t = torch.from_numpy(ytr).view(-1,1).to(device)\n",
    "        Xva_t = torch.from_numpy(Xva).to(device)\n",
    "        yva_t = torch.from_numpy(yva).view(-1,1).to(device)\n",
    "\n",
    "        model = MLP(Xtr.shape[1]).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        hist = []\n",
    "        t0 = time()\n",
    "        for ep in range(1, args.epochs+1):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            pred = model(Xtr_t)\n",
    "            loss = loss_fn(pred, ytr_t)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = loss_fn(model(Xva_t), yva_t).item()\n",
    "            hist.append(val_loss)\n",
    "            if ep % 10 == 0 or ep == 1:\n",
    "                print(f\"epoch={ep} val_loss={val_loss:.4f}\")\n",
    "        print(f\"Training time: {time()-t0:.2f}s on {device}\")\n",
    "\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"[INFO] Saved model: {model_path}\")\n",
    "\n",
    "        # metrics.json and eval_history.csv\n",
    "        import json\n",
    "        metrics = {\n",
    "            \"final_val_loss\": float(hist[-1]) if hist else None,\n",
    "            \"epochs\": int(args.epochs),\n",
    "            \"learning_rate\": float(args.learning_rate),\n",
    "            \"train_rows\": int(Xtr.shape[0]),\n",
    "            \"val_rows\": int(Xva.shape[0]),\n",
    "            \"features\": list(range(Xtr.shape[1])),\n",
    "            \"model_uri\": model_path,\n",
    "            \"device\": str(device),\n",
    "        }\n",
    "        from io import StringIO\n",
    "        _write_text(f\"{art_dir}/metrics.json\", json.dumps(metrics, indent=2))\n",
    "        csv = \"iter,val_loss\\n\" + \"\\n\".join(f\"{i+1},{v}\" for i, v in enumerate(hist))\n",
    "        _write_text(f\"{art_dir}/eval_history.csv\", csv)\n",
    "    finally:\n",
    "        # persist log and restore streams\n",
    "        try:\n",
    "            _write_text(log_path, buf.getvalue())\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] could not write log: {e}\")\n",
    "        sys.stdout, sys.stderr = orig_out, orig_err\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Launch the training job (no base_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845644eb",
   "metadata": {},
   "source": [
    "python\n",
    "RUN_ID = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ARTIFACT_DIR = f\"gs://{BUCKET_NAME}/artifacts/pytorch/{RUN_ID}\"\n",
    "MODEL_URI = f\"{ARTIFACT_DIR}/model.pt\"   # model + metrics + logs will live here together\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=f\"pytorch_nn_{RUN_ID}\",\n",
    "    script_path=\"GCP_helpers/train_nn.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-cpu.2-1:latest\",  # or pytorch-gpu.2-1\n",
    "    requirements=[\"torch\", \"numpy\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        f\"--epochs=200\",\n",
    "        f\"--learning_rate=0.001\",\n",
    "        f\"--model_out={MODEL_URI}\",   # drives where *all* artifacts go\n",
    "    ],\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",  # CPU fine for small datasets\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(\"Artifacts folder:\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9de21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "**What you’ll see in `gs://…/artifacts/pytorch/<RUN_ID>/`:**\n",
    "- `model.pt` — PyTorch weights (`state_dict`).\n",
    "- `metrics.json` — final val loss, hyperparameters, dataset sizes, device, model URI.\n",
    "- `eval_history.csv` — per‑epoch validation loss (for plots/regression checks).\n",
    "- `training.log` — complete stdout/stderr for reproducibility and debugging.\n",
    "\n",
    "## Optional: GPU training\n",
    "\n",
    "For larger models or heavier data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33316a",
   "metadata": {},
   "source": [
    "python\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=f\"pytorch_nn_gpu_{RUN_ID}\",\n",
    "    script_path=\"GCP_helpers/train_nn.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-1:latest\",\n",
    "    requirements=[\"torch\", \"numpy\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        f\"--epochs=200\",\n",
    "        f\"--learning_rate=0.001\",\n",
    "        f\"--model_out={MODEL_URI}\",\n",
    "    ],\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    sync=True,\n",
    ")\n",
    "```\n",
    "\n",
    "GPU tips:\n",
    "- On small problems, GPU startup/transfer overhead can erase speedups—benchmark before you scale.\n",
    "- Stick to a single replica unless your batch sizes and dataset really warrant data parallelism.\n",
    "\n",
    "## Distributed training (when to consider)\n",
    "\n",
    "- **Data parallelism** (DDP) helps when a single GPU is saturated by batch size/throughput. For most workshop‑scale models, a single machine/GPU is simpler and cheaper.\n",
    "- **Model parallelism** is for very large networks that don’t fit on one device—overkill for this lesson.\n",
    "\n",
    "## Monitoring jobs & finding outputs\n",
    "\n",
    "- Console → Vertex AI → Training → Custom Jobs → your run → “Output directory” shows the container logs and the environment’s `AIP_MODEL_DIR`.\n",
    "- Your script writes **model + metrics + eval history + training.log** next to `--model_out`, e.g., `gs://<bucket>/artifacts/pytorch/<RUN_ID>/`.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Use **CustomTrainingJob** with a prebuilt PyTorch container; let your script control outputs via `--model_out`.\n",
    "- Keep artifacts **together** (model, metrics, history, log) in one folder for reproducibility.\n",
    "- `.npz` speeds up loading and plays nicely with PyTorch.\n",
    "- Start on CPU for small datasets; use GPU only when profiling shows a clear win.\n",
    "- Skip `base_output_dir` unless you specifically want Vertex’s default run directory; staging bucket is just for the SDK packaging tarball.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
