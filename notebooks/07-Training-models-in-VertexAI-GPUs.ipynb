{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a09d52",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Training Models in Vertex AI: PyTorch Example\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- When should you consider a GPU (or TPU) instance for PyTorch training in Vertex AI, and what are the trade‑offs for small vs. large workloads?\n",
    "- How do you launch a script‑based training job and write **all** artifacts (model, metrics, logs) next to each other in GCS without deploying a managed model?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Prepare the Titanic dataset and save train/val arrays to compressed `.npz` files in GCS.\n",
    "- Submit a *CustomTrainingJob* that runs a PyTorch script and explicitly writes outputs to a chosen `gs://…/artifacts/.../` folder.\n",
    "- Co‑locate artifacts: `model.pt` (or `.joblib`), `metrics.json`, `eval_history.csv`, and `training.log` for reproducibility.\n",
    "- Choose CPU vs. GPU instances sensibly; understand when distributed training is (not) worth it.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Initial setup\n",
    "\n",
    "#### 1. Open pre-filled notebook\n",
    "Navigate to `/Intro_GCP_for_ML/notebooks/06-Training-models-in-VertexAI-GPUs.ipynb` to begin this notebook. Select the *PyTorch* environment (kernel) Local PyTorch is only needed for local tests. Your *Vertex AI job* uses the container specified by `container_uri` (e.g., `pytorch-cpu.2-1` or `pytorch-gpu.2-1`), so it brings its own framework at run time.\n",
    "\n",
    "#### 2. CD to instance home directory\n",
    "To ensure we're all in the saming starting spot, change directory to your Jupyter home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ee07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9137c6a",
   "metadata": {},
   "source": [
    "#### 3. Set environment variables \n",
    "This code initializes the Vertex AI environment by importing the Python SDK, setting the project, region, and defining a GCS bucket for input/output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, storage\n",
    "client = storage.Client()\n",
    "PROJECT_ID = client.project\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"sinkorswim-johndoe-titanic\" # ADJUST to your bucket's name\n",
    "\n",
    "print(f\"project = {PROJECT_ID}\\nregion = {REGION}\\nbucket = {BUCKET_NAME}\")\n",
    "\n",
    "# initializes the Vertex AI environment with the correct project and location. Staging bucket is used for storing the compressed software that's packaged for training/tuning jobs.\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}/.vertex_staging\") # store tar balls in staging folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd174b",
   "metadata": {},
   "source": [
    "## Prepare data as `.npz`\n",
    "\n",
    "Why `.npz`? NumPy's `.npz` files are compressed binary containers that can store multiple arrays (e.g., features and labels) together in a single file. They offer numerous benefits:\n",
    "\n",
    "- Smaller, faster I/O than CSV for arrays.  \n",
    "- One file can hold multiple arrays (`X_train`, `y_train`).\n",
    "- Natural fit for `torch.utils.data.Dataset` / `DataLoader`.  \n",
    "- **Cloud-friendly:** compressed `.npz` files reduce upload and download times and minimize GCS egress costs. Because each `.npz` is a single binary object, reading it from Google Cloud Storage (GCS) requires only one network call—much faster and cheaper than streaming many small CSVs or images individually.  \n",
    "- **Efficient data movement:** when you launch a Vertex AI training job, GCS objects referenced in your script (for example, `gs://.../train_data.npz`) are automatically staged to the job's VM or container at runtime. Vertex copies these objects into its local scratch disk before execution, so subsequent reads (e.g., `np.load(...)`) occur from local storage rather than directly over the network. For small-to-medium datasets, this happens transparently and incurs minimal startup delay.  \n",
    "- **Reproducible binary format:** unlike CSV, `.npz` preserves exact dtypes and shapes, ensuring identical results across different environments and containers.  \n",
    "- Each GCS object read or listing request incurs a small per-request cost; using a single `.npz` reduces both the number of API calls and associated latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ea4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load Titanic CSV (from local or GCS you've already downloaded to the notebook)\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(\"titanic_train.csv\")\n",
    "df = pd.read_csv(io.BytesIO(blob.download_as_bytes()))\n",
    "\n",
    "# Minimal preprocessing to numeric arrays\n",
    "sex_enc = LabelEncoder().fit(df[\"Sex\"])            # Fit label encoder on 'Sex' column (male/female)\n",
    "df[\"Sex\"] = sex_enc.transform(df[\"Sex\"])           # Convert 'Sex' to numeric values (e.g., male=1, female=0)\n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")       # Replace missing embarkation ports with most common ('S')\n",
    "emb_enc = LabelEncoder().fit(df[\"Embarked\"])       # Fit label encoder on 'Embarked' column (S/C/Q)\n",
    "df[\"Embarked\"] = emb_enc.transform(df[\"Embarked\"]) # Convert embarkation categories to numeric codes\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())   # Fill missing ages with median (robust to outliers)\n",
    "df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())# Fill missing fares with median to avoid NaNs\n",
    "\n",
    "X = df[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]].values  # Select numeric feature columns as input\n",
    "y = df[\"Survived\"].values                                                # Target variable (1=survived, 0=did not survive)\n",
    "\n",
    "scaler = StandardScaler()                                                # Initialize standard scaler for standardization (best practice for neural net training)\n",
    "X = scaler.fit_transform(X)                                              # Scale features to mean=0, std=1 for stable training\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(                       # Split dataset into training and validation sets\n",
    "    X, y, test_size=0.2, random_state=42)                                # 80% training, 20% validation (fixed random seed)\n",
    "\n",
    "np.savez(\"/home/jupyter/train_data.npz\", X_train=X_train, y_train=y_train)             # Save training arrays to compressed .npz file\n",
    "np.savez(\"/home/jupyter/val_data.npz\",   X_val=X_val,   y_val=y_val)                   # Save validation arrays to compressed .npz file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a897d",
   "metadata": {},
   "source": [
    "We can then upload the files to our GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GCS\n",
    "bucket.blob(\"data/train_data.npz\").upload_from_filename(\"/home/jupyter/train_data.npz\")\n",
    "bucket.blob(\"data/val_data.npz\").upload_from_filename(\"/home/jupyter/val_data.npz\")\n",
    "print(\"Uploaded: gs://%s/data/train_data.npz and val_data.npz\" % BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652078f4",
   "metadata": {},
   "source": [
    "To check our work (bucket contents), we can again use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes = 0\n",
    "# bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "for blob in client.list_blobs(BUCKET_NAME):\n",
    "    total_size_bytes += blob.size\n",
    "    print(blob.name)\n",
    "\n",
    "total_size_mb = total_size_bytes / (1024**2)\n",
    "print(f\"Total size of bucket '{BUCKET_NAME}': {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b6077",
   "metadata": {},
   "source": [
    "## Minimal PyTorch training script (`train_nn.py`) - local test\n",
    "\n",
    "**Outside of this workshop, you should run these kinds of tests on your local laptop or lab PC when possible.** We're using the Workbench VM here only for convenience in this workshop setting, but this does incur a small fee for our running VM. \n",
    "\n",
    "- For large datasets, use a small representative sample of the total dataset when testing locally (i.e., just to verify that code is working and model overfits nearly perfectly after training enough epochs)\n",
    "- For larger models, use smaller model equivalents (e.g., 100M vs 7B params) when testing locally\n",
    "  \n",
    "Find this file in our repo: `Intro_GCP_for_ML/scripts/train_nn.py`. It does three things:\n",
    "1) loads `.npz` from local or GCS\n",
    "2) trains a tiny multilayer perceptron (MLP)\n",
    "3) writes all outputs side‑by‑side (model + metrics + eval history + training.log) to the same `--model_out` folder.\n",
    "\n",
    "To test this code, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ec44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training hyperparameters to use in all model training runs downstream\n",
    "MAX_EPOCHS = 500\n",
    "LR =  0.001\n",
    "PATIENCE = 50\n",
    "\n",
    "# local training run\n",
    "import time as t\n",
    "\n",
    "start = t.time()\n",
    "\n",
    "# Example: run your custom training script with args\n",
    "!python /home/jupyter/Intro_GCP_for_ML/scripts/train_nn.py \\\n",
    "    --train /home/jupyter/train_data.npz \\\n",
    "    --val /home/jupyter/val_data.npz \\\n",
    "    --epochs $MAX_EPOCHS \\\n",
    "    --learning_rate $LR \\\n",
    "    --patience $PATIENCE\n",
    "\n",
    "print(f\"Total local runtime: {t.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819bc9e",
   "metadata": {},
   "source": [
    "If applicable (numpy mismatch), run the below code after uncommenting it (select code and type `Ctrl+/` for multiline uncommenting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix numpy mismatch\n",
    "# !pip install --upgrade --force-reinstall \"numpy<2\"\n",
    "\n",
    "# # Then, rerun:\n",
    "\n",
    "# import time as t\n",
    "\n",
    "# start = t.time()\n",
    "\n",
    "# # Example: run your custom training script with args\n",
    "# !python /home/jupyter/Intro_GCP_for_ML/scripts/train_nn.py \\\n",
    "#     --train /home/jupyter/train_data.npz \\\n",
    "#     --val /home/jupyter/val_data.npz \\\n",
    "#    --epochs $MAX_EPOCHS \\\n",
    "#    --learning_rate $LR \\\n",
    "#    --patience $PATIENCE\n",
    "\n",
    "\n",
    "# print(f\"Total local runtime: {t.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a40792",
   "metadata": {},
   "source": [
    "### Reproducibility test\n",
    "Without reproducibility, it's impossible to gain reliable insights into the efficacy of our methods. An essential component of applied ML/AI is ensuring our experiments are reproducible. Let's first rerun the same code we did above to verify we get the same result. \n",
    "\n",
    "* Take a look near the top of `Intro_GCP_for_ML/scripts/train_nn.py` where we are setting multiple numpy and torch seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c71be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "start = t.time()\n",
    "\n",
    "# Example: run your custom training script with args\n",
    "!python /home/jupyter/Intro_GCP_for_ML/scripts/train_nn.py \\\n",
    "    --train /home/jupyter/train_data.npz \\\n",
    "    --val /home/jupyter/val_data.npz \\\n",
    "    --epochs $MAX_EPOCHS \\\n",
    "    --learning_rate $LR \\\n",
    "    --patience $PATIENCE\n",
    "\n",
    "print(f\"Total local runtime: {t.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8faed",
   "metadata": {},
   "source": [
    "**Please don't use cloud resources for code that is not reproducible!**\n",
    "\n",
    "### Evaluate the locally trained model on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a94cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, numpy as np\n",
    "sys.path.append(\"/home/jupyter/Intro_GCP_for_ML/scripts\")\n",
    "from train_nn import TitanicNet\n",
    "\n",
    "# load validation data\n",
    "d = np.load(\"/home/jupyter/val_data.npz\")\n",
    "X_val, y_val = d[\"X_val\"], d[\"y_val\"]\n",
    "\n",
    "# rebuild model and load weights\n",
    "m = TitanicNet()\n",
    "state = torch.load(\"/home/jupyter/model.pt\", map_location=\"cpu\") \n",
    "m.load_state_dict(state)\n",
    "m.eval()\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    # model already outputs probabilities in (0,1) because final layer is Sigmoid\n",
    "    probs = m(X_val_t).squeeze(1)              # shape [N]\n",
    "    preds = (probs >= 0.5).long().cpu().numpy()\n",
    "\n",
    "acc = (preds == y_val).mean()\n",
    "print(f\"Local model val accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11345b",
   "metadata": {},
   "source": [
    "We should see an accuracy that matches our best epoch in the local training run. Note that in our setup, early stopping is based on validation loss; not accuracy.\n",
    "\n",
    "## Launch the training job \n",
    "\n",
    "In the previous episode, we trained an XGBoost model using Vertex AI's CustomTrainingJob interface. Here, we'll do the same for a PyTorch neural network. The structure is nearly identical —  we define a training script, select a prebuilt container (CPU or GPU), and specify where to write all outputs in Google Cloud Storage (GCS). The main difference is that PyTorch requires us to save our own model weights and metrics inside the script rather than relying on Vertex to package a model automatically.\n",
    "\n",
    "### Set training job configuration vars\n",
    "For our image, we can find the corresponding PyTorch image by visiting: [cloud.google.com/vertex-ai/docs/training/pre-built-containers#pytorch](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "RUN_ID = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ARTIFACT_DIR = f\"gs://{BUCKET_NAME}/artifacts/pytorch/{RUN_ID}\"\n",
    "IMAGE = 'us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest' # cpu-only version\n",
    "MACHINE = \"n1-standard-4\" # CPU fine for small datasets\n",
    "\n",
    "print(f\"RUN_ID = {RUN_ID}\\nARTIFACT_DIR = {ARTIFACT_DIR}\\nMACHINE = {MACHINE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c8b65",
   "metadata": {},
   "source": [
    "### Init the training job with configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c48988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init job (this does not consume any resources)\n",
    "LAST_NAME = 'DOE' # REPLACE with your last name. Since we're in a shared account envirnoment, this will help us track down jobs in the Console\n",
    "DISPLAY_NAME = f\"{LAST_NAME}_pytorch_nn_{RUN_ID}\" \n",
    "\n",
    "# init the job. This does not consume resources until we run job.run()\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    script_path=\"Intro_GCP_for_ML/scripts/train_nn.py\",\n",
    "    container_uri=IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c32c5f",
   "metadata": {},
   "source": [
    "### Run the job, paying for our `MACHINE` on-demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        f\"--epochs={MAX_EPOCHS}\",\n",
    "        f\"--learning_rate={LR}\",\n",
    "        f\"--patience={PATIENCE}\",\n",
    "    ],\n",
    "    replica_count=1,\n",
    "    machine_type=MACHINE,\n",
    "    base_output_dir=ARTIFACT_DIR,  # sets AIP_MODEL_DIR used by your script\n",
    "    sync=True,\n",
    ")\n",
    "print(\"Artifacts folder:\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4364e",
   "metadata": {},
   "source": [
    "#### Monitoring training jobs in the Console\n",
    "1. Go to the Google Cloud Console.  \n",
    "2. Navigate to **Vertex AI > Training > Custom Jobs**.  \n",
    "3. Click on your job name to see status, logs, and output model artifacts.  \n",
    "4. Cancel jobs from the console if needed (be careful not to stop jobs you don't own in shared projects).\n",
    "\n",
    "**Quick link**: https://console.cloud.google.com/vertex-ai/training/training-pipelines?hl=en&project=doit-rci-mlm25-4626\n",
    "\n",
    "Check our bucket contents to verify expected outputs are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes = 0\n",
    "# bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "for blob in client.list_blobs(BUCKET_NAME):\n",
    "    total_size_bytes += blob.size\n",
    "    print(blob.name)\n",
    "\n",
    "total_size_mb = total_size_bytes / (1024**2)\n",
    "print(f\"Total size of bucket '{BUCKET_NAME}': {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c3612",
   "metadata": {},
   "source": [
    "**What you'll see in `gs://…/artifacts/pytorch/<RUN_ID>/`:**\n",
    "\n",
    "- `model.pt` — PyTorch weights (`state_dict`).\n",
    "- `metrics.json` — final val loss, hyperparameters, dataset sizes, device, model URI.\n",
    "- `eval_history.csv` — per‑epoch validation loss (for plots/regression checks).\n",
    "- `training.log` — complete stdout/stderr for reproducibility and debugging.\n",
    "\n",
    "### Evaluate the Vertex-trained model on the validation data\n",
    "\n",
    "We can check out work to see if this model gives the same result as our \"locally\" trained model above. First, we'll copy the model from GCS to our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fb6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model.pt from GCS (replace RUN_ID with your run folder)\n",
    "!gsutil cp {ARTIFACT_DIR}/model/model.pt /home/jupyter/model_vertex.pt\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f4eb4",
   "metadata": {},
   "source": [
    "As before, we can run our model evaluation code with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06179ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, numpy as np\n",
    "sys.path.append(\"/home/jupyter/Intro_GCP_for_ML/scripts\")\n",
    "from train_nn import TitanicNet\n",
    "\n",
    "# load validation data\n",
    "d = np.load(\"/home/jupyter/val_data.npz\")\n",
    "X_val, y_val = d[\"X_val\"], d[\"y_val\"]\n",
    "\n",
    "# rebuild model and load weights\n",
    "m = TitanicNet()\n",
    "state = torch.load(\"/home/jupyter/model_vertex.pt\", map_location=\"cpu\")  \n",
    "m.load_state_dict(state)\n",
    "m.eval()\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    # model already outputs probabilities in (0,1) because final layer is Sigmoid\n",
    "    probs = m(X_val_t).squeeze(1)              # shape [N]\n",
    "    preds = (probs >= 0.5).long().cpu().numpy()\n",
    "\n",
    "acc = (preds == y_val).mean()\n",
    "print(f\"Vertex model val accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b687f",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Training on Vertex AI\n",
    "\n",
    "In the previous example, we ran our PyTorch training job on a CPU-only machine using the `pytorch-cpu` container. That setup works well for small models or quick tests since CPU instances are cheaper and start faster.\n",
    "\n",
    "In this section, we'll attach a GPU to our Vertex AI training job to speed up heavier workloads. The workflow is nearly identical to the CPU version, except for a few changes:\n",
    "\n",
    "- The container image switches to the GPU-enabled version (`pytorch-gpu.2-4.py310:latest`), which includes CUDA and cuDNN.\n",
    "- The machine type (`n1-standard-8`) defines CPU and memory resources, while we now add a GPU accelerator (`NVIDIA_TESLA_T4`, `NVIDIA_L4`, etc.). **For guidance on selecting a machine type and accelerator, visit the [Compute for ML](https://qualiamachine.github.io/Intro_GCP_for_ML/instances-for-ML.html) resource.**\n",
    "- The training script, arguments, and artifact handling all stay the same.\n",
    "\n",
    "This makes it easy to start with a CPU run for testing, then scale up to GPU training by changing only the image and adding accelerator parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8171c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "LAST_NAME = \"DOE\"  # Your last name goes in the job display name so it's easy to find in the Console\n",
    "RUN_ID = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# GCS folder where ALL artifacts (model.pt, metrics.json, eval_history.csv, training.log) will be saved.\n",
    "# Your train_nn.py writes to AIP_MODEL_DIR, and base_output_dir (below) sets that variable for the job.\n",
    "ARTIFACT_DIR = f\"gs://{BUCKET_NAME}/artifacts/pytorch/{RUN_ID}\"\n",
    "\n",
    "# ---- Container image ----\n",
    "# Use a prebuilt TRAINING image that has PyTorch + CUDA. This enables GPU at runtime.\n",
    "IMAGE = \"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-4.py310:latest\"\n",
    "\n",
    "# ---- Machine vs Accelerator (important!) ----\n",
    "# machine_type = the VM's CPU/RAM shape. It is NOT a GPU by itself.\n",
    "# We often pick n1-standard-8 as a balanced baseline for single-GPU jobs.\n",
    "MACHINE = \"n1-standard-8\"\n",
    "\n",
    "# To actually get a GPU, you *attach* one via accelerator_type + accelerator_count.\n",
    "# Common choices:\n",
    "#   \"NVIDIA_TESLA_T4\" (cost-effective, widely available)\n",
    "#   \"NVIDIA_L4\"       (newer, CUDA 12.x, good perf/$)\n",
    "#   \"NVIDIA_TESLA_V100\" / \"NVIDIA_A100_40GB\" (high-end, pricey)\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT = 1  # Increase (2,4) only if your code supports multi-GPU (e.g., DDP)\n",
    "\n",
    "# Alternative (GPU-bundled) machines:\n",
    "# If you pick an A2 type like \"a2-highgpu-1g\", it already includes 1 A100 GPU.\n",
    "# In that case, you can omit accelerator_type/accelerator_count entirely.\n",
    "# Example:\n",
    "# MACHINE = \"a2-highgpu-1g\"\n",
    "# (and then remove the accelerator_* kwargs in job.run)\n",
    "\n",
    "print(\n",
    "    \"RUN_ID =\", RUN_ID,\n",
    "    \"\\nARTIFACT_DIR =\", ARTIFACT_DIR,\n",
    "    \"\\nIMAGE =\", IMAGE,\n",
    "    \"\\nMACHINE =\", MACHINE,\n",
    "    \"\\nACCELERATOR_TYPE =\", ACCELERATOR_TYPE,\n",
    "    \"\\nACCELERATOR_COUNT =\", ACCELERATOR_COUNT,\n",
    ")\n",
    "\n",
    "DISPLAY_NAME = f\"{LAST_NAME}_pytorch_nn_{RUN_ID}\"\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    script_path=\"Intro_GCP_for_ML/scripts/train_nn.py\",  # Your PyTorch trainer\n",
    "    container_uri=IMAGE,  # Must be a *training* image (not prediction)\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        f\"--epochs={MAX_EPOCHS}\",\n",
    "        f\"--learning_rate={LR}\",\n",
    "        f\"--patience={PATIENCE}\",\n",
    "    ],\n",
    "    replica_count=1,                 # One worker (simple, cheaper)\n",
    "    machine_type=MACHINE,            # CPU/RAM shape of the VM (no GPU implied)\n",
    "    accelerator_type=ACCELERATOR_TYPE,   # Attaches the selected GPU model\n",
    "    accelerator_count=ACCELERATOR_COUNT, # Number of GPUs to attach\n",
    "    base_output_dir=ARTIFACT_DIR,    # Sets AIP_MODEL_DIR used by your script for all artifacts\n",
    "    sync=True,                       # Waits for job to finish so you can inspect outputs immediately\n",
    ")\n",
    "\n",
    "print(\"Artifacts folder:\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653cc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model.pt from GCS (replace RUN_ID with your run folder)\n",
    "!gsutil cp {ARTIFACT_DIR}/model/model.pt /home/jupyter/model_vertexGPU.pt\n",
    "!ls\n",
    "import sys, torch, numpy as np\n",
    "sys.path.append(\"/home/jupyter/Intro_GCP_for_ML/scripts\")\n",
    "from train_nn import TitanicNet\n",
    "\n",
    "# load validation data\n",
    "d = np.load(\"/home/jupyter/val_data.npz\")\n",
    "X_val, y_val = d[\"X_val\"], d[\"y_val\"]\n",
    "\n",
    "# rebuild model and load weights\n",
    "m = TitanicNet()\n",
    "state = torch.load(\"/home/jupyter/model_vertexGPU.pt\", map_location=\"cpu\")  \n",
    "m.load_state_dict(state)\n",
    "m.eval()\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    # model already outputs probabilities in (0,1) because final layer is Sigmoid\n",
    "    probs = m(X_val_t).squeeze(1)              # shape [N]\n",
    "    preds = (probs >= 0.5).long().cpu().numpy()\n",
    "\n",
    "acc = (preds == y_val).mean()\n",
    "print(f\"Vertex model val accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659cd43",
   "metadata": {},
   "source": [
    "GPU tips:\n",
    "- On small problems, GPU startup/transfer overhead can erase speedups—benchmark before you scale.\n",
    "- Stick to a single replica unless your batch sizes and dataset really warrant data parallelism.\n",
    "\n",
    "## Distributed training (when to consider)\n",
    "\n",
    "- **Data parallelism** (DDP) helps when a single GPU is saturated by batch size/throughput. For most workshop‑scale models, a single machine/GPU is simpler and cheaper.\n",
    "- **Model parallelism** is for very large networks that don't fit on one device—overkill for this lesson.\n",
    "\n",
    "## Additional resources\n",
    "To learn more about PyTorch and Vertex AI integrations, visit the docs: [docs.cloud.google.com/vertex-ai/docs/start/pytorch](https://docs.cloud.google.com/vertex-ai/docs/start/pytorch)\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Use **CustomTrainingJob** with a prebuilt PyTorch container; let your script control outputs via `--model_out`.\n",
    "- Keep artifacts **together** (model, metrics, history, log) in one folder for reproducibility.\n",
    "- `.npz` speeds up loading and plays nicely with PyTorch.\n",
    "- Start on CPU for small datasets; use GPU only when profiling shows a clear win.\n",
    "- Skip `base_output_dir` unless you specifically want Vertex's default run directory; staging bucket is just for the SDK packaging tarball.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
