{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb7e87d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Training Models in Vertex AI: PyTorch Example\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- When should you consider a GPU (or TPU) instance for PyTorch training in Vertex AI, and what are the trade‑offs for small vs. large workloads?\n",
    "- How do you launch a script‑based training job and write **all** artifacts (model, metrics, logs) next to each other in GCS without deploying a managed model?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Prepare the Titanic dataset and save train/val arrays to compressed `.npz` files in GCS.\n",
    "- Submit a **CustomTrainingJob** that runs a PyTorch script and explicitly writes outputs to a chosen `gs://…/artifacts/.../` folder.\n",
    "- Co‑locate artifacts: `model.pt` (or `.joblib`), `metrics.json`, `eval_history.csv`, and `training.log` for reproducibility.\n",
    "- Choose CPU vs. GPU instances sensibly; understand when distributed training is (not) worth it.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Initial setup\n",
    "\n",
    "#### 1. Open pre-filled notebook\n",
    "Navigate to `/Intro_GCP_for_ML/notebooks/06-Training-models-in-VertexAI-GPUs.ipynb` to begin this notebook. Select the *PyTorch* environment (kernel) Local PyTorch is only needed for local tests. Your *Vertex AI job* uses the container specified by `container_uri` (e.g., `pytorch-cpu.2-1` or `pytorch-gpu.2-1`), so it brings its own framework at run time.\n",
    "\n",
    "#### 2. CD to instance home directory\n",
    "To ensure we're all in the saming starting spot, change directory to your Jupyter home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892cf88",
   "metadata": {},
   "source": [
    "#### 3. Set environment variables \n",
    "This code initializes the Vertex AI environment by importing the Python SDK, setting the project, region, and defining a GCS bucket for input/output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e34aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, storage\n",
    "client = storage.Client()\n",
    "PROJECT_ID = client.project\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"sinkorswim-johndoe-titanic\" # ADJUST to your bucket's name\n",
    "\n",
    "print(f\"project = {PROJECT_ID}\\nregion = {REGION}\\nbucket = {BUCKET_NAME}\")\n",
    "\n",
    "# initializes the Vertex AI environment with the correct project and location. Staging bucket is used for storing the compressed software that's packaged for training/tuning jobs.\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}/.vertex_staging\") # store tar balls in staging folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41eff81",
   "metadata": {},
   "source": [
    "## Prepare data as `.npz`\n",
    "\n",
    "Why `.npz`? NumPy's `.npz` files are compressed binary containers that can store multiple arrays (e.g., features and labels) together in a single file. They offer numerous benefits:\n",
    "\n",
    "- Smaller, faster I/O than CSV for arrays.  \n",
    "- One file can hold multiple arrays (`X_train`, `y_train`).\n",
    "- Natural fit for `torch.utils.data.Dataset` / `DataLoader`.  \n",
    "- **Cloud-friendly:** compressed `.npz` files reduce upload and download times and minimize GCS egress costs. Because each `.npz` is a single binary object, reading it from Google Cloud Storage (GCS) requires only one network call—much faster and cheaper than streaming many small CSVs or images individually.  \n",
    "- **Efficient data movement:** when you launch a Vertex AI training job, GCS objects referenced in your script (for example, `gs://.../train_data.npz`) are automatically staged to the job’s VM or container at runtime. Vertex copies these objects into its local scratch disk before execution, so subsequent reads (e.g., `np.load(...)`) occur from local storage rather than directly over the network. For small-to-medium datasets, this happens transparently and incurs minimal startup delay.  \n",
    "- **Reproducible binary format:** unlike CSV, `.npz` preserves exact dtypes and shapes, ensuring identical results across different environments and containers.  \n",
    "- Each GCS object read or listing request incurs a small per-request cost; using a single `.npz` reduces both the number of API calls and associated latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load Titanic CSV (from local or GCS you've already downloaded to the notebook)\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(\"titanic_train.csv\")\n",
    "df = pd.read_csv(io.BytesIO(blob.download_as_bytes()))\n",
    "\n",
    "# Minimal preprocessing to numeric arrays\n",
    "sex_enc = LabelEncoder().fit(df[\"Sex\"])            # Fit label encoder on 'Sex' column (male/female)\n",
    "df[\"Sex\"] = sex_enc.transform(df[\"Sex\"])           # Convert 'Sex' to numeric values (e.g., male=1, female=0)\n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")       # Replace missing embarkation ports with most common ('S')\n",
    "emb_enc = LabelEncoder().fit(df[\"Embarked\"])       # Fit label encoder on 'Embarked' column (S/C/Q)\n",
    "df[\"Embarked\"] = emb_enc.transform(df[\"Embarked\"]) # Convert embarkation categories to numeric codes\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())   # Fill missing ages with median (robust to outliers)\n",
    "df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())# Fill missing fares with median to avoid NaNs\n",
    "\n",
    "X = df[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]].values  # Select numeric feature columns as input\n",
    "y = df[\"Survived\"].values                                                # Target variable (1=survived, 0=did not survive)\n",
    "\n",
    "scaler = StandardScaler()                                                # Initialize standard scaler for standardization (best practice for neural net training)\n",
    "X = scaler.fit_transform(X)                                              # Scale features to mean=0, std=1 for stable training\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(                       # Split dataset into training and validation sets\n",
    "    X, y, test_size=0.2, random_state=42)                                # 80% training, 20% validation (fixed random seed)\n",
    "\n",
    "np.savez(\"/home/jupyter/train_data.npz\", X_train=X_train, y_train=y_train)             # Save training arrays to compressed .npz file\n",
    "np.savez(\"/home/jupyter/val_data.npz\",   X_val=X_val,   y_val=y_val)                   # Save validation arrays to compressed .npz file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c56c8",
   "metadata": {},
   "source": [
    "We can then upload the files to our GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GCS\n",
    "bucket.blob(\"data/train_data.npz\").upload_from_filename(\"/home/jupyter/train_data.npz\")\n",
    "bucket.blob(\"data/val_data.npz\").upload_from_filename(\"/home/jupyter/val_data.npz\")\n",
    "print(\"Uploaded: gs://%s/data/train_data.npz and val_data.npz\" % BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2e45f",
   "metadata": {},
   "source": [
    "## Minimal PyTorch training script (`train_nn.py`)\n",
    "\n",
    "Find this file in our repo: `Intro_GCP_for_ML/scripts/train_nn.py`. It does three things:\n",
    "1) loads `.npz` from local or GCS\n",
    "2) trains a tiny multilayer perceptron (MLP)\n",
    "3) **writes all outputs side‑by‑side** (model + metrics + eval history + training.log) to the same `--model_out` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488eea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "start = t.time()\n",
    "\n",
    "# Example: run your custom training script with args\n",
    "!python /home/jupyter/Intro_GCP_for_ML/scripts/train_nn.py \\\n",
    "    --train /home/jupyter/train_data.npz \\\n",
    "    --val /home/jupyter/val_data.npz \\\n",
    "    --epochs 50 \\\n",
    "    --learning_rate 0.001\n",
    "\n",
    "print(f\"Total local runtime: {t.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba380e9a",
   "metadata": {},
   "source": [
    "To address the numpy mismatch, we can run the following code first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ed230",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dea990",
   "metadata": {},
   "source": [
    "Then, rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "start = t.time()\n",
    "\n",
    "# Example: run your custom training script with args\n",
    "!python /home/jupyter/Intro_GCP_for_ML/scripts/train_nn.py \\\n",
    "    --train /home/jupyter/train_data.npz \\\n",
    "    --val /home/jupyter/val_data.npz \\\n",
    "    --epochs 50 \\\n",
    "    --learning_rate 0.001\n",
    "\n",
    "print(f\"Total local runtime: {t.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016474a",
   "metadata": {},
   "source": [
    "## Launch the training job (no base_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f468a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ARTIFACT_DIR = f\"gs://{BUCKET_NAME}/artifacts/pytorch/{RUN_ID}\"\n",
    "MODEL_URI = f\"{ARTIFACT_DIR}/model.pt\"   # model + metrics + logs will live here together\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=f\"pytorch_nn_{RUN_ID}\",\n",
    "    script_path=\"GCP_helpers/train_nn.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-cpu.2-1:latest\",  # or pytorch-gpu.2-1\n",
    "    requirements=[\"torch\", \"numpy\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        f\"--epochs=200\",\n",
    "        f\"--learning_rate=0.001\",\n",
    "        f\"--model_out={MODEL_URI}\",   # drives where *all* artifacts go\n",
    "    ],\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",  # CPU fine for small datasets\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(\"Artifacts folder:\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49261184",
   "metadata": {},
   "source": [
    "**What you’ll see in `gs://…/artifacts/pytorch/<RUN_ID>/`:**\n",
    "- `model.pt` — PyTorch weights (`state_dict`).\n",
    "- `metrics.json` — final val loss, hyperparameters, dataset sizes, device, model URI.\n",
    "- `eval_history.csv` — per‑epoch validation loss (for plots/regression checks).\n",
    "- `training.log` — complete stdout/stderr for reproducibility and debugging.\n",
    "\n",
    "## Optional: GPU training\n",
    "\n",
    "For larger models or heavier data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=f\"pytorch_nn_gpu_{RUN_ID}\",\n",
    "    script_path=\"GCP_helpers/train_nn.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-1:latest\",\n",
    "    requirements=[\"torch\", \"numpy\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        f\"--epochs=200\",\n",
    "        f\"--learning_rate=0.001\",\n",
    "        f\"--model_out={MODEL_URI}\",\n",
    "    ],\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2e56e",
   "metadata": {},
   "source": [
    "GPU tips:\n",
    "- On small problems, GPU startup/transfer overhead can erase speedups—benchmark before you scale.\n",
    "- Stick to a single replica unless your batch sizes and dataset really warrant data parallelism.\n",
    "\n",
    "## Distributed training (when to consider)\n",
    "\n",
    "- **Data parallelism** (DDP) helps when a single GPU is saturated by batch size/throughput. For most workshop‑scale models, a single machine/GPU is simpler and cheaper.\n",
    "- **Model parallelism** is for very large networks that don’t fit on one device—overkill for this lesson.\n",
    "\n",
    "## Monitoring jobs & finding outputs\n",
    "\n",
    "- Console → Vertex AI → Training → Custom Jobs → your run → “Output directory” shows the container logs and the environment’s `AIP_MODEL_DIR`.\n",
    "- Your script writes **model + metrics + eval history + training.log** next to `--model_out`, e.g., `gs://<bucket>/artifacts/pytorch/<RUN_ID>/`.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Use **CustomTrainingJob** with a prebuilt PyTorch container; let your script control outputs via `--model_out`.\n",
    "- Keep artifacts **together** (model, metrics, history, log) in one folder for reproducibility.\n",
    "- `.npz` speeds up loading and plays nicely with PyTorch.\n",
    "- Start on CPU for small datasets; use GPU only when profiling shows a clear win.\n",
    "- Skip `base_output_dir` unless you specifically want Vertex’s default run directory; staging bucket is just for the SDK packaging tarball.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
