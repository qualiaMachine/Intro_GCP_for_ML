{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f51c0e0",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hyperparameter Tuning in Vertex AI: Neural Network Example\"\n",
    "teaching: 60\n",
    "exercises: 0\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can we efficiently manage hyperparameter tuning in Vertex AI?  \n",
    "- How can we parallelize tuning jobs to optimize time without increasing costs?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Set up and run a hyperparameter tuning job in Vertex AI.  \n",
    "- Define search spaces for `ContinuousParameter` and `CategoricalParameter`.  \n",
    "- Log and capture objective metrics for evaluating tuning success.  \n",
    "- Optimize tuning setup to balance cost and efficiency, including parallelization.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "To conduct efficient hyperparameter tuning with neural networks (or any model) in Vertex AI, we’ll use Vertex AI’s **Hyperparameter Tuning Jobs**. The key is defining a clear search space, ensuring metrics are properly logged, and keeping costs manageable by controlling the number of trials and level of parallelization.\n",
    "\n",
    "### Key steps for hyperparameter tuning\n",
    "\n",
    "The overall process involves these steps:\n",
    "\n",
    "1. Prepare training script and ensure metrics are logged.  \n",
    "2. Define hyperparameter search space.  \n",
    "3. Configure a hyperparameter tuning job in Vertex AI.  \n",
    "4. Set data paths and launch the tuning job.  \n",
    "5. Monitor progress in the Vertex AI Console.  \n",
    "6. Extract best model and evaluate.  \n",
    "\n",
    "#### 0. Directory setup\n",
    "Change directory to your Jupyter home folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca3c30",
   "metadata": {},
   "source": [
    "#### 1. Prepare training script with metric logging\n",
    "Your training script (`train_nn.py`) should periodically print validation accuracy in a format that Vertex AI can capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88188bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (epoch + 1) % 100 == 0 or epoch == epochs - 1:\n",
    "    print(f\"validation_accuracy: {val_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fb21c",
   "metadata": {},
   "source": [
    "Vertex AI automatically captures metrics logged in this format (`key: value`).  \n",
    "\n",
    "#### 2. Define hyperparameter search space\n",
    "\n",
    "In Vertex AI, you specify hyperparameter ranges when configuring the tuning job. You can define both discrete and continuous ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61815d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_spec = {\n",
    "    \"epochs\": aiplatform.hyperparameter_tuning_utils.IntegerParameterSpec(min=100, max=1000, scale=\"linear\"),\n",
    "    \"learning_rate\": aiplatform.hyperparameter_tuning_utils.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281fd06",
   "metadata": {},
   "source": [
    "- **IntegerParameterSpec**: Defines integer ranges.  \n",
    "- **DoubleParameterSpec**: Defines continuous ranges, with optional scaling.  \n",
    "\n",
    "#### 3. Configure hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=\"pytorch-train-hpt\",\n",
    "    script_path=\"GCP_helpers/train_nn.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13:latest\",\n",
    "    requirements=[\"torch\", \"pandas\", \"numpy\", \"scikit-learn\"],\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-13:latest\",\n",
    ")\n",
    "\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"pytorch-hpt-job\",\n",
    "    custom_job=job,\n",
    "    metric_spec={\"validation_accuracy\": \"maximize\"},\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=4,\n",
    "    parallel_trial_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35c330",
   "metadata": {},
   "source": [
    "#### 4. Launch the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_job.run(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    args=[\n",
    "        \"--train=gs://{}/train_data.npz\".format(BUCKET_NAME),\n",
    "        \"--val=gs://{}/val_data.npz\".format(BUCKET_NAME),\n",
    "        \"--epochs=100\",\n",
    "        \"--learning_rate=0.001\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897d285",
   "metadata": {},
   "source": [
    "- **max_trial_count**: Total number of configurations tested.  \n",
    "- **parallel_trial_count**: Number of trials run at once (recommend ≤4 to let adaptive search improve).  \n",
    "\n",
    "#### 5. Monitor tuning job in Vertex AI Console\n",
    "1. Navigate to **Vertex AI > Training > Hyperparameter tuning jobs**.  \n",
    "2. View trial progress, logs, and metrics.  \n",
    "3. Cancel jobs from the console if needed.  \n",
    "\n",
    "#### 6. Extract and evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = hpt_job.trials[0]  # Best trial listed first after completion\n",
    "print(\"Best hyperparameters:\", best_trial.parameters)\n",
    "print(\"Best objective value:\", best_trial.final_measurement.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1afb41",
   "metadata": {},
   "source": [
    "You can then load the best model artifact from the associated GCS path and evaluate on test data.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: discussion\n",
    "\n",
    "### What is the effect of parallelism in tuning?  \n",
    "\n",
    "- How might running 10 trials in parallel differ from running 2 at a time in terms of cost, time, and quality of results?  \n",
    "- When would you want to prioritize speed over adaptive search benefits?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Vertex AI Hyperparameter Tuning Jobs let you efficiently explore parameter spaces using adaptive strategies.  \n",
    "- Always test with `max_trial_count=1` first to confirm your setup works.  \n",
    "- Limit `parallel_trial_count` to a small number (2–4) to benefit from adaptive search.  \n",
    "- Use GCS for input/output and monitor jobs through the Vertex AI Console.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
