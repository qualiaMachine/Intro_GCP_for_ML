{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9690214d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hyperparameter Tuning in Vertex AI: Neural Network Example\"\n",
    "teaching: 60\n",
    "exercises: 0\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can we efficiently manage hyperparameter tuning in Vertex AI?  \n",
    "- How can we parallelize tuning jobs to optimize time without increasing costs?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Set up and run a hyperparameter tuning job in Vertex AI.  \n",
    "- Define search spaces for `ContinuousParameter` and `CategoricalParameter`.  \n",
    "- Log and capture objective metrics for evaluating tuning success.  \n",
    "- Optimize tuning setup to balance cost and efficiency, including parallelization.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "To conduct efficient hyperparameter tuning with neural networks (or any model) in Vertex AI, we’ll use Vertex AI’s Hyperparameter Tuning Jobs. The key is defining a clear search space, ensuring metrics are properly logged, and keeping costs manageable by controlling the number of trials and level of parallelization.\n",
    "\n",
    "### Key steps for hyperparameter tuning\n",
    "\n",
    "The overall process involves these steps:\n",
    "\n",
    "1. Prepare the training script and ensure metrics are logged.  \n",
    "2. Define the hyperparameter search space.  \n",
    "3. Configure a hyperparameter tuning job in Vertex AI.  \n",
    "4. Set data paths and launch the tuning job.  \n",
    "5. Monitor progress in the Vertex AI Console.  \n",
    "6. Extract the best model and inspect recorded metrics.  \n",
    "\n",
    "#### 0. Directory setup\n",
    "Change to your Jupyter home folder to keep paths consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca443dae",
   "metadata": {},
   "source": [
    "#### 1. Prepare training script with metric logging\n",
    "Your training script (`train_nn.py`) should periodically print validation metrics in a format Vertex AI can capture. Vertex AI parses lines like `key: value` from stdout.\n",
    "\n",
    "Add these two lines right after you compute `val_loss` and `val_acc` inside the epoch loop (the patch below shows exactly where):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d68d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"validation_loss: {val_loss:.6f}\", flush=True)\n",
    "print(f\"validation_accuracy: {val_acc:.6f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d54eb",
   "metadata": {},
   "source": [
    "This is in addition to your existing human-readable line (e.g., `epoch=... val_loss:... val_acc:...`).  \n",
    "Patch file you can apply: `train_nn.patch` (provided below). The current script also writes a `metrics.json` with keys like `final_val_accuracy` which we will read later. fileciteturn0file0\n",
    "\n",
    "#### 2. Define hyperparameter search space\n",
    "This step defines which parameters Vertex AI will vary across trials and their allowed ranges. The number of total settings tested is determined later using `max_trial_count`.\n",
    "\n",
    "Include early-stopping parameters so the tuner can learn good stopping behavior for your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb25e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "parameter_spec = {\n",
    "    \"epochs\": hpt.IntegerParameterSpec(min=100, max=300, scale=\"linear\"),\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=1e-4, max=1e-1, scale=\"log\"),\n",
    "    \"patience\": hpt.IntegerParameterSpec(min=5, max=20, scale=\"linear\"),\n",
    "    \"min_delta\": hpt.DoubleParameterSpec(min=0.0, max=0.01, scale=\"linear\"),  # improvement threshold on val_loss\n",
    "    # You can also add restore_best as categorical if you want to compare behaviors:\n",
    "    # \"restore_best\": hpt.CategoricalParameterSpec(values=[\"true\",\"false\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4eecc",
   "metadata": {},
   "source": [
    "#### 3. Initialize Vertex AI, project, and bucket\n",
    "Initialize the Vertex AI SDK and set your staging and artifact locations in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e664c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, storage\n",
    "import datetime as dt\n",
    "\n",
    "client = storage.Client()\n",
    "PROJECT_ID = client.project\n",
    "REGION = \"us-central1\"\n",
    "LAST_NAME = \"DOE\"  # change to your name or unique ID\n",
    "BUCKET_NAME = \"sinkorswim-johndoe-titanic\"  # replace with your bucket name\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}/.vertex_staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdeb42",
   "metadata": {},
   "source": [
    "#### 4. Define runtime configuration\n",
    "Create a unique run ID and set the container, machine type, and base output directory for artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "BASE_DIR = f\"gs://{BUCKET_NAME}/artifacts/pytorch_hpt/{RUN_ID}\"\n",
    "\n",
    "IMAGE = \"us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest\"  # CPU example\n",
    "MACHINE = \"n1-standard-4\"\n",
    "ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "ACCELERATOR_COUNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77bcc73",
   "metadata": {},
   "source": [
    "#### 5. Configure hyperparameter tuning job\n",
    "Set the optimization metric to the printed key `validation_accuracy`. Start with one trial to validate your setup before scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_spec = {\"validation_accuracy\": \"maximize\"}  # matches script print key\n",
    "\n",
    "custom_job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name=f\"{LAST_NAME}_pytorch_hpt-trial_{RUN_ID}\",\n",
    "    script_path=\"Intro_GCP_for_ML/scripts/train_nn.py\",\n",
    "    container_uri=IMAGE,\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        \"--epochs=200\",                 # HPT will override when sampling\n",
    "        \"--learning_rate=0.001\",        # HPT will override when sampling\n",
    "        \"--patience=10\",                # HPT will override when sampling\n",
    "        \"--min_delta=0.0\",              # HPT will override when sampling\n",
    "        # \"--restore_best=true\",        # optional categorical param if enabled above\n",
    "    ],\n",
    "    base_output_dir=BASE_DIR,\n",
    "    machine_type=MACHINE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")\n",
    "\n",
    "DISPLAY_NAME = f\"{LAST_NAME}_pytorch_hpt_{RUN_ID}\"\n",
    "\n",
    "tuning_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    custom_job=custom_job,                 # must be a CustomJob (not CustomTrainingJob)\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=10,                    # controls how many configurations are tested\n",
    "    parallel_trial_count=2,                # how many run concurrently (keep small for adaptive search)\n",
    ")\n",
    "\n",
    "tuning_job.run(sync=True)\n",
    "print(\"HPT artifacts base:\", BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6b351",
   "metadata": {},
   "source": [
    "#### 6. Monitor tuning job\n",
    "Open **Vertex AI → Training → Hyperparameter tuning jobs** to track trials, parameters, and metrics. You can also stop jobs from the console if needed.\n",
    "\n",
    "#### 7. Inspect best trial results\n",
    "After completion, look up the best configuration and objective value from the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d366ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuning_job.trials[0]  # best-first\n",
    "print(\"Best hyperparameters:\", best_trial.parameters)\n",
    "print(\"Best validation_accuracy:\", best_trial.final_measurement.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38578bfc",
   "metadata": {},
   "source": [
    "#### 8. Review recorded metrics in GCS\n",
    "Your script writes a `metrics.json` (with keys such as `final_val_accuracy`, `final_val_loss`) to each trial’s output directory (under `BASE_DIR`). The snippet below aggregates those into a dataframe for side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7fa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import json, pandas as pd\n",
    "\n",
    "def list_metrics_from_gcs(base_dir: str):\n",
    "    client = storage.Client()\n",
    "    bucket_name = base_dir.replace(\"gs://\", \"\").split(\"/\")[0]\n",
    "    prefix = \"/\".join(base_dir.replace(\"gs://\", \"\").split(\"/\")[1:])\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "\n",
    "    records = []\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\"metrics.json\"):\n",
    "            trial_id = blob.name.split(\"/\")[-2]\n",
    "            data = json.loads(blob.download_as_text())\n",
    "            data[\"trial_id\"] = trial_id\n",
    "            records.append(data)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = list_metrics_from_gcs(BASE_DIR)\n",
    "print(df[[\"trial_id\",\"final_val_accuracy\",\"final_val_loss\",\"best_val_loss\",\"best_epoch\",\"patience\",\"min_delta\",\"learning_rate\"]].sort_values(\"final_val_accuracy\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0957130",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::: discussion\n",
    "\n",
    "### What is the effect of parallelism in tuning?  \n",
    "\n",
    "- How might running 10 trials in parallel differ from running 2 at a time in terms of cost, time, and result quality?  \n",
    "- When would you want to prioritize speed over adaptive search benefits?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Vertex AI Hyperparameter Tuning Jobs efficiently explore parameter spaces using adaptive strategies.  \n",
    "- Define parameter ranges in `parameter_spec`; the number of settings tried is controlled later by `max_trial_count`.  \n",
    "- Keep the printed metric name consistent with `metric_spec` (here: `validation_accuracy`).  \n",
    "- Limit `parallel_trial_count` (2–4) to help adaptive search.  \n",
    "- Use GCS for input/output and aggregate `metrics.json` across trials for detailed analysis.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
