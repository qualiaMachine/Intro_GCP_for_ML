{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e2f55c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hyperparameter Tuning in Vertex AI: Neural Network Example\"\n",
    "teaching: 60\n",
    "exercises: 0\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can we efficiently manage hyperparameter tuning in Vertex AI?  \n",
    "- How can we parallelize tuning jobs to optimize time without increasing costs?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Set up and run a hyperparameter tuning job in Vertex AI.  \n",
    "- Define search spaces for `ContinuousParameter` and `CategoricalParameter`.  \n",
    "- Log and capture objective metrics for evaluating tuning success.  \n",
    "- Optimize tuning setup to balance cost and efficiency, including parallelization.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "To conduct efficient hyperparameter tuning with neural networks (or any model) in Vertex AI, we'll use Vertex AI's Hyperparameter Tuning Jobs. The key is defining a clear search space, ensuring metrics are properly logged, and keeping costs manageable by controlling the number of trials and level of parallelization.\n",
    "\n",
    "### Key steps for hyperparameter tuning\n",
    "\n",
    "The overall process involves these steps:\n",
    "\n",
    "1. Prepare the training script and ensure metrics are logged.  \n",
    "2. Define the hyperparameter search space.  \n",
    "3. Configure a hyperparameter tuning job in Vertex AI.  \n",
    "4. Set data paths and launch the tuning job.  \n",
    "5. Monitor progress in the Vertex AI Console.  \n",
    "6. Extract the best model and inspect recorded metrics.  \n",
    "\n",
    "#### 0. Initial setup\n",
    "\n",
    "Navigate to `/Intro_GCP_for_ML/notebooks/08-Hyperparameter-tuning.ipynb` to begin this notebook. Select the *PyTorch* environment (kernel) Local PyTorch is only needed for local tests. Your *Vertex AI job* uses the container specified by `container_uri` (e.g., `pytorch-cpu.2-1` or `pytorch-gpu.2-1`), so it brings its own framework at run time.\n",
    "\n",
    "Change to your Jupyter home folder to keep paths consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1bcd75",
   "metadata": {},
   "source": [
    "#### 1. Prepare training script with metric logging\n",
    "Your training script (`train_nn.py`) should periodically print validation metrics in a format Vertex AI can capture. Vertex AI parses lines like `key: value` from stdout.\n",
    "\n",
    "Add these two lines right after you compute `val_loss` and `val_acc` inside the epoch loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"validation_loss: {val_loss:.6f}\", flush=True)\n",
    "print(f\"validation_accuracy: {val_acc:.6f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8148a0",
   "metadata": {},
   "source": [
    "#### 2. Define hyperparameter search space\n",
    "This step defines which parameters Vertex AI will vary across trials and their allowed ranges. The number of total settings tested is determined later using `max_trial_count`.\n",
    "\n",
    "Vertex AI uses **Bayesian optimization** by default (internally listed as `\"ALGORITHM_UNSPECIFIED\"` in the API).  That means if you don’t explicitly specify a search algorithm, Vertex AI automatically applies an adaptive Bayesian strategy to balance exploration (trying new areas of the parameter space) and exploitation (focusing near the best results so far).  Each completed trial helps the tuner model how your objective metric (for example, `validation_accuracy`) changes across parameter values. Subsequent trials then sample new parameter combinations that are statistically more likely to improve performance, which usually yields better results than random or grid search—especially when `max_trial_count` is limited.\n",
    "\n",
    "Include early-stopping parameters so the tuner can learn good stopping behavior for your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0728f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=1e-4, max=1e-1, scale=\"log\"),\n",
    "    \"patience\": hpt.IntegerParameterSpec(min=5, max=20, scale=\"linear\"),\n",
    "    \"min_delta\": hpt.DoubleParameterSpec(min=1e-6, max=1e-3, scale=\"log\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7aa58c",
   "metadata": {},
   "source": [
    "#### 3. Initialize Vertex AI, project, and bucket\n",
    "Initialize the Vertex AI SDK and set your staging and artifact locations in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, storage\n",
    "import datetime as dt\n",
    "\n",
    "client = storage.Client()\n",
    "PROJECT_ID = client.project\n",
    "REGION = \"us-central1\"\n",
    "LAST_NAME = \"DOE\"  # change to your name or unique ID\n",
    "BUCKET_NAME = \"sinkorswim-johndoe-titanic\"  # replace with your bucket name\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}/.vertex_staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c783e",
   "metadata": {},
   "source": [
    "#### 4. Define runtime configuration\n",
    "Create a unique run ID and set the container, machine type, and base output directory for artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ARTIFACT_DIR = f\"gs://{BUCKET_NAME}/artifacts/pytorch_hpt/{RUN_ID}\"\n",
    "\n",
    "IMAGE = \"us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest\"  # CPU example\n",
    "MACHINE = \"n1-standard-4\"\n",
    "ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "ACCELERATOR_COUNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabec1d",
   "metadata": {},
   "source": [
    "#### 5. Configure hyperparameter tuning job\n",
    "When you use Vertex AI Hyperparameter Tuning Jobs, each trial needs a complete, runnable training configuration: the script, its arguments, the container image, and the compute environment.  \n",
    "Rather than defining these pieces inline each time, we create a **CustomJob** to hold that configuration.  \n",
    "\n",
    "The CustomJob acts as the blueprint for running a single training task — specifying exactly what to run and on what resources. The tuner then reuses that job definition across all trials, automatically substituting in new hyperparameter values for each run.  \n",
    "\n",
    "This approach has a few practical advantages:\n",
    "\n",
    "- You only define the environment once — machine type, accelerators, and output directories are all reused across trials.  \n",
    "- The tuner can safely inject trial-specific parameters (those declared in `parameter_spec`) while leaving other arguments unchanged.  \n",
    "- It provides a clean separation between *what a single job does* (`CustomJob`) and *how many times to repeat it with new settings* (`HyperparameterTuningJob`).  \n",
    "- It avoids the extra abstraction layers of higher-level wrappers like `CustomTrainingJob`, which automatically package code and environments. Using `CustomJob.from_local_script` keeps the workflow predictable and explicit.\n",
    "\n",
    "In short:  \n",
    "`CustomJob` defines how to run one training run.  \n",
    "`HyperparameterTuningJob` defines how to repeat it with different parameter sets and track results.  \n",
    "\n",
    "The number of total runs is set by `max_trial_count`, and the number of simultaneous runs is controlled by `parallel_trial_count`.  Each trial's output and metrics are logged under the GCS `base_output_dir`. **ALWAYS START WITH 1 trial** before scaling up `max_trial_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_spec = {\"validation_accuracy\": \"maximize\"}  # matches script print key\n",
    "\n",
    "custom_job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name=f\"{LAST_NAME}_pytorch_hpt-trial_{RUN_ID}\",\n",
    "    script_path=\"/home/jupyter/Intro_GCP_for_ML/scripts/train_nn.py\",\n",
    "    container_uri=IMAGE,\n",
    "    args=[\n",
    "        f\"--train=gs://{BUCKET_NAME}/data/train_data.npz\",\n",
    "        f\"--val=gs://{BUCKET_NAME}/data/val_data.npz\",\n",
    "        \"--learning_rate=0.001\",        # HPT will override when sampling\n",
    "        \"--patience=10\",                # HPT will override when sampling\n",
    "    ],\n",
    "    base_output_dir=ARTIFACT_DIR,\n",
    "    machine_type=MACHINE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")\n",
    "\n",
    "DISPLAY_NAME = f\"{LAST_NAME}_pytorch_hpt_{RUN_ID}\"\n",
    "\n",
    "# ALWAYS START WITH 1 trial before scaling up `max_trial_count`\n",
    "tuning_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    custom_job=custom_job,                 # must be a CustomJob (not CustomTrainingJob)\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=1,                    # controls how many configurations are tested\n",
    "    parallel_trial_count=1,                # how many run concurrently (keep small for adaptive search)\n",
    "    # search_algorithm=\"ALGORITHM_UNSPECIFIED\",  # default = adaptive search (Bayesian)\n",
    "    # search_algorithm=\"RANDOM_SEARCH\",          # optional override\n",
    "    # search_algorithm=\"GRID_SEARCH\",            # optional override\n",
    ")\n",
    "\n",
    "tuning_job.run(sync=True)\n",
    "print(\"HPT artifacts base:\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00a65d",
   "metadata": {},
   "source": [
    "#### 6. Monitor tuning job\n",
    "Open **Vertex AI → Training → Hyperparameter tuning jobs** to track trials, parameters, and metrics. You can also stop jobs from the console if needed. For MLM25, the folllowing link should work: [https://console.cloud.google.com/vertex-ai/training/hyperparameter-tuning-jobs?hl=en&project=doit-rci-mlm25-4626]([https://console.cloud.google.com/vertex-ai/training/hyperparameter-tuning-jobs?hl=en&project=doit-rci-mlm25-4626]).\n",
    "\n",
    "#### 7. Inspect best trial results\n",
    "After completion, look up the best configuration and objective value from the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuning_job.trials[0]  # best-first\n",
    "print(\"Best hyperparameters:\", best_trial.parameters)\n",
    "print(\"Best validation_accuracy:\", best_trial.final_measurement.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b531b0c",
   "metadata": {},
   "source": [
    "#### 8. Review recorded metrics in GCS\n",
    "Your script writes a `metrics.json` (with keys such as `final_val_accuracy`, `final_val_loss`) to each trial's output directory (under `ARTIFACT_DIR`). The snippet below aggregates those into a dataframe for side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import json, pandas as pd\n",
    "\n",
    "def list_metrics_from_gcs(ARTIFACT_DIR: str):\n",
    "    client = storage.Client()\n",
    "    bucket_name = ARTIFACT_DIR.replace(\"gs://\", \"\").split(\"/\")[0]\n",
    "    prefix = \"/\".join(ARTIFACT_DIR.replace(\"gs://\", \"\").split(\"/\")[1:])\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "\n",
    "    records = []\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\"metrics.json\"):\n",
    "            trial_id = blob.name.split(\"/\")[-2]\n",
    "            data = json.loads(blob.download_as_text())\n",
    "            data[\"trial_id\"] = trial_id\n",
    "            records.append(data)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = list_metrics_from_gcs(ARTIFACT_DIR)\n",
    "print(df[[\"trial_id\",\"final_val_accuracy\",\"final_val_loss\",\"best_val_loss\",\"best_epoch\",\"patience\",\"min_delta\",\"learning_rate\"]].sort_values(\"final_val_accuracy\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9041f4",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::: discussion\n",
    "\n",
    "### What is the effect of parallelism in tuning?  \n",
    "\n",
    "- How might running 10 trials in parallel differ from running 2 at a time in terms of cost, time, and result quality?  \n",
    "- When would you want to prioritize speed over adaptive search benefits?  \n",
    "\n",
    "**Cost:**  \n",
    "- If you run the same total number of trials, total cost is *roughly unchanged*; you're paying for the same amount of compute, just compressed into a shorter wall-clock window.  \n",
    "- Parallelism can raise short-term spend rate (more machines running at once) and may increase idle/overhead if trials start/finish unevenly.\n",
    "\n",
    "**Time:**  \n",
    "- Higher `parallel_trial_count` reduces wall-clock time almost linearly until you hit queue, quota, or data/IO bottlenecks.  \n",
    "- Startup overhead (image pull, environment setup) is paid for each concurrent trial; with many short trials, this overhead can become a larger fraction of runtime.\n",
    "\n",
    "**Result quality (adaptive search):**  \n",
    "- Vertex AI's adaptive search benefits from learning from early trials.  \n",
    "- With many trials in flight simultaneously, the tuner can't incorporate results quickly, so it explores “blind” for longer. This often yields slightly *worse* final results for a fixed `max_trial_count`.  \n",
    "- With modest parallelism (e.g., 2–4), the tuner can still update beliefs and exploit promising regions sooner.\n",
    "\n",
    "**Guidelines:**  \n",
    "- Start small: `parallel_trial_count` in the range 2–4 is a good default.  \n",
    "- Keep parallelism to **≤ 25–33%** of `max_trial_count` when you care about adaptive quality.  \n",
    "- Increase parallelism when your trials are long and you're confident the search space is well-bounded (less need for rapid adaptation).\n",
    "\n",
    "**When to prioritize speed (higher parallelism):**  \n",
    "- Strict deadlines or demo timelines.  \n",
    "- Very cheap/short trials where startup time dominates.  \n",
    "- You're using a non-adaptive or nearly random search space.  \n",
    "- You have unused quota/credits and want faster iteration.\n",
    "\n",
    "**When to prioritize adaptive quality (lower parallelism):**  \n",
    "- Trials are expensive, noisy, or have high variance; learning from early wins saves budget.  \n",
    "- Small `max_trial_count` (e.g., ≤ 10–20).  \n",
    "- Early stopping is enabled and you want the tuner to exploit promising regions quickly.  \n",
    "- You're adding new dimensions (e.g., LR + patience + min_delta) and want the search to refine intelligently.\n",
    "\n",
    "**Practical recipe:**  \n",
    "- First run: `max_trial_count=1`, `parallel_trial_count=1` (pipeline sanity check).  \n",
    "- Main run: `max_trial_count=10–20`, `parallel_trial_count=2–4`.  \n",
    "- Scale up parallelism only after the above completes cleanly and you confirm adaptive performance is acceptable.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Vertex AI Hyperparameter Tuning Jobs efficiently explore parameter spaces using adaptive strategies.  \n",
    "- Define parameter ranges in `parameter_spec`; the number of settings tried is controlled later by `max_trial_count`.  \n",
    "- Keep the printed metric name consistent with `metric_spec` (here: `validation_accuracy`).  \n",
    "- Limit `parallel_trial_count` (2–4) to help adaptive search.  \n",
    "- Use GCS for input/output and aggregate `metrics.json` across trials for detailed analysis.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
