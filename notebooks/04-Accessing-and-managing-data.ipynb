{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c77fa27",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Accessing and Managing Data in GCS with Vertex AI Notebooks\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can I load data from GCS into a Vertex AI Workbench notebook?  \n",
    "- How do I monitor storage usage and costs for my GCS bucket?  \n",
    "- What steps are involved in pushing new data back to GCS from a notebook?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Read data directly from a GCS bucket into memory in a Vertex AI notebook.  \n",
    "- Check storage usage and estimate costs for data in a GCS bucket.  \n",
    "- Upload new files from the Vertex AI environment back to the GCS bucket.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Initial setup \n",
    "\n",
    "#### Load pre-filled Jupyter notebooks \n",
    "If you haven't opened your newly created VM from the last episode yet, lick **Open JupyterLab** to open the instance in Jupyter Lab. From there, we can create as many Jupyter notebooks as we would like within the instance environment. \n",
    "\n",
    "We will then select the standard python3 environment to start our first .ipynb notebook (Jupyter notebook). We can use this environment since we aren't doing any training/tuning just yet.\n",
    "\n",
    "Within the Jupyter notebook, run the following command to clone the lesson repo into our Jupyter environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/qualiaMachine/Intro_GCP_for_ML.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad95004",
   "metadata": {},
   "source": [
    "Then, navigate to `/Intro_GCP_for_ML/notebooks/Accessing-and-managing-data.ipynb` to begin the first notebook.\n",
    "\n",
    "#### Set up GCP environment\n",
    "Before interacting with GCS, we need to authenticate and initialize the client libraries. This ensures our notebook can talk to GCP securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io\n",
    "client = storage.Client()\n",
    "print(\"Project:\", client.project)\n",
    "print(\"Credentials:\", client._credentials.service_account_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db222cc4",
   "metadata": {},
   "source": [
    "## Reading data from GCS\n",
    "\n",
    "As with S3, you can either (A) read data directly from GCS into memory, or (B) download a copy into your notebook VM. Since we’re using notebooks as controllers rather than training environments, the recommended approach is **reading directly from GCS**.\n",
    "\n",
    "### A) Reading data directly into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14694d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"yourname_titanic\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(\"titanic_train.csv\")\n",
    "train_data = pd.read_csv(io.BytesIO(blob.download_as_bytes()))\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19da53",
   "metadata": {},
   "source": [
    "### B) Downloading a local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"yourname-titanic-gcs\"\n",
    "blob_name = \"titanic_train.csv\"\n",
    "local_path = \"/home/jupyter/titanic_train.csv\"\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "blob.download_to_filename(local_path)\n",
    "\n",
    "!ls -lh /home/jupyter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0a211",
   "metadata": {},
   "source": [
    "## Checking storage usage of a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes = 0\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name):\n",
    "    total_size_bytes += blob.size\n",
    "\n",
    "total_size_mb = total_size_bytes / (1024**2)\n",
    "print(f\"Total size of bucket '{bucket_name}': {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129a51e",
   "metadata": {},
   "source": [
    "## Estimating storage costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_price_per_gb = 0.02  # $/GB/month for Standard storage\n",
    "total_size_gb = total_size_bytes / (1024**3)\n",
    "monthly_cost = total_size_gb * storage_price_per_gb\n",
    "\n",
    "print(f\"Estimated monthly cost: ${monthly_cost:.4f}\")\n",
    "print(f\"Estimated annual cost: ${monthly_cost*12:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e645670",
   "metadata": {},
   "source": [
    "For updated prices, see [GCS Pricing](https://cloud.google.com/storage/pricing).\n",
    "\n",
    "## Writing output files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample file locally on the notebook VM\n",
    "with open(\"Notes.txt\", \"w\") as f:\n",
    "    f.write(\"This is a test note for GCS.\")\n",
    "\n",
    "# Point to the right bucket\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Create a *Blob* object, which represents a path inside the bucket\n",
    "# (here it will end up as gs://<bucket_name>/docs/Notes.txt)\n",
    "blob = bucket.blob(\"docs/Notes.txt\")\n",
    "\n",
    "# Upload the local file into that blob (object) in GCS\n",
    "blob.upload_from_filename(\"Notes.txt\")\n",
    "\n",
    "print(\"File uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2b311",
   "metadata": {},
   "source": [
    "List bucket contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a264d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blob in client.list_blobs(bucket_name):\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e27bd",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::::: challenge\n",
    "\n",
    "### Challenge: Estimating GCS Costs\n",
    "\n",
    "Suppose you store **50 GB** of data in Standard storage (us-central1) for one month.  \n",
    "- Estimate the monthly storage cost.  \n",
    "- Then estimate the cost if you download (egress) the entire dataset once at the end of the month.  \n",
    "\n",
    "**Hints**  \n",
    "- Storage: $0.02 per GB-month  \n",
    "- Egress: $0.12 per GB  \n",
    "\n",
    ":::::::::::::::: solution\n",
    "\n",
    "- Storage cost: 50 GB × $0.02 = $1.00  \n",
    "- Egress cost: 50 GB × $0.12 = $6.00  \n",
    "- **Total cost: $7.00 for one month including one full download**  \n",
    "\n",
    ":::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints \n",
    "\n",
    "- Load data from GCS into memory to avoid managing local copies when possible.  \n",
    "- Periodically check storage usage and costs to manage your GCS budget.  \n",
    "- Use Vertex AI Workbench notebooks to upload analysis results back to GCS, keeping workflows organized and reproducible.  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
