{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5e063d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Retrieval-Augmented Generation (RAG) with Vertex AI\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How do we go from \"a pile of PDFs\" to \"ask a question and get a cited answer\" using Google Cloud tools?\n",
    "- What are the key parts of a RAG system (chunking, embedding, retrieval, generation), and how do they map onto Vertex AI services?\n",
    "- How much does each part of this pipeline cost (VM time, embeddings, LLM calls), and where can we keep it cheap?\n",
    "- Can we use open models / Hugging Face instead of Google models, and what does that change?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Unpack the core RAG pipeline: ingest → chunk → embed → retrieve → answer.\n",
    "- Run a minimal, fully programmatic RAG loop on a Vertex AI Workbench VM using Google’s own foundation models (for embeddings + generation).\n",
    "- Understand how to substitute open-source / Hugging Face models if you want to avoid managed API costs.\n",
    "- Answer questions using content from provided papers and return citations instead of vibes.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Overview: What we're building\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a pattern:\n",
    "\n",
    "1. You ask a question.  \n",
    "2. The system **retrieves** relevant passages from your PDFs or data.  \n",
    "3. An LLM **answers** using those passages only, with citations.\n",
    "\n",
    "This approach powers sustainability-related projects like **WattBot**, which extracts AI water and energy metrics from research papers.\n",
    "\n",
    "**Cost mindset:**  \n",
    "- **VM cost:** pay for Workbench instance uptime. Stop when not in use.  \n",
    "- **Embedding cost:** pay per character embedded — only once per doc.  \n",
    "- **Generation cost:** pay per token for input + output. Shorter prompts = cheaper.  \n",
    "\n",
    "**Hugging Face alternatives:**  \n",
    "You can replace Google-managed APIs with open models such as:  \n",
    "- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`, `BAAI/bge-large-en-v1.5`  \n",
    "- **Generators:** `google/gemma-2b-it`, `mistralai/Mistral-7B-Instruct`, or `tiiuae/falcon-7b-instruct`  \n",
    "However, this requires a GPU or large CPU VM (e.g., `n1-standard-8` + `T4`) and manual model management.  \n",
    "Vertex AI’s managed models (`text-embedding-004`, `gemini-2.5-flash-001`) are cost-optimized and scalable — better for workshops or low-ops setups.\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade google-cloud-aiplatform google-cloud-storage vertexai pypdf scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5d0ff",
   "metadata": {},
   "source": [
    "**Cost note:** Installing packages is free; you're only billed for VM runtime.\n",
    "\n",
    "### Initialize project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from vertexai import init as vertexai_init\n",
    "import os\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"<YOUR_PROJECT_ID>\")\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "vertexai_init(project=PROJECT_ID, location=REGION)\n",
    "print(\"Initialized:\", PROJECT_ID, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe85fea",
   "metadata": {},
   "source": [
    "## Step 2: Extract and chunk PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, pathlib, re, pandas as pd\n",
    "from pypdf import PdfReader\n",
    "\n",
    "ZIP_PATH = pathlib.Path(\"Intro_GCP_VertexAI/data/pdfs_bundle.zip\")\n",
    "DOC_DIR = pathlib.Path(\"./docs\")\n",
    "DOC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# unzip\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    zf.extractall(DOC_DIR)\n",
    "\n",
    "def chunk_text(text, max_chars=1200, overlap=150):\n",
    "    for i in range(0, len(text), max_chars - overlap):\n",
    "        yield text[i:i+max_chars]\n",
    "\n",
    "rows = []\n",
    "for pdf in DOC_DIR.glob(\"*.pdf\"):\n",
    "    txt = \"\"\n",
    "    for page in PdfReader(str(pdf)).pages:\n",
    "        txt += page.extract_text() or \"\"\n",
    "    for i, chunk in enumerate(chunk_text(re.sub(r\"\\s+\", \" \", txt))):\n",
    "        rows.append({\"doc\": pdf.name, \"chunk_id\": i, \"text\": chunk})\n",
    "\n",
    "import pandas as pd\n",
    "corpus_df = pd.DataFrame(rows)\n",
    "print(len(corpus_df), \"chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61c595",
   "metadata": {},
   "source": [
    "**Cost note:** Only VM runtime applies. Chunk size affects future embedding cost.\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Embed text using Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import numpy as np\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    vecs = []\n",
    "    for batch in [texts[i:i+32] for i in range(0, len(texts), 32)]:\n",
    "        resp = model.get_embeddings(batch)\n",
    "        for r in resp: vecs.append(r.values)\n",
    "    return np.array(vecs, dtype=\"float32\")\n",
    "\n",
    "emb_matrix = get_embeddings(corpus_df.text.tolist())\n",
    "print(\"Embeddings shape:\", emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfde86",
   "metadata": {},
   "source": [
    "**Cost note:** This is the first paid step — roughly $0.0001–0.0002 per 1k tokens.  \n",
    "Cache embeddings locally or in GCS to avoid recharging later.\n",
    "\n",
    "\n",
    "\n",
    "## Step 4: Retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39556b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(metric=\"cosine\", n_neighbors=5)\n",
    "nn.fit(emb_matrix)\n",
    "\n",
    "def retrieve(query):\n",
    "    q_vec = model.get_embeddings([query])[0].values\n",
    "    dist, idx = nn.kneighbors([q_vec], return_distance=True)\n",
    "    df = corpus_df.iloc[idx[0]].copy()\n",
    "    df[\"similarity\"] = 1 - dist[0]\n",
    "    return df.sort_values(\"similarity\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc8d6c",
   "metadata": {},
   "source": [
    "**Cost note:** Retrieval is free (runs locally). Each new query triggers one embedding call.\n",
    "\n",
    "\n",
    "\n",
    "## Step 5: Generate answers using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7510fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "gmodel = GenerativeModel(\"gemini-2.5-flash-001\")\n",
    "\n",
    "def ask(query, top_k=5):\n",
    "    hits = retrieve(query).head(top_k)\n",
    "    context = \"\\n\\n\".join([f\"[{r.doc}#chunk-{r.chunk_id}] {r.text}\" for _, r in hits.iterrows()])\n",
    "    prompt = f\"You are a sustainability analyst. Use only the following context to answer the question.\\n\\n{context}\\n\\nQ: {query}\\nA:\"\n",
    "    ans = gmodel.generate_content(prompt)\n",
    "    return ans.text\n",
    "\n",
    "print(ask(\"What water usage (WUE) is reported for model training?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d27ab",
   "metadata": {},
   "source": [
    "**Cost note:** Gemini Flash costs ~$0.00005 per 1k input tokens and ~$0.0002 per 1k output tokens.  \n",
    "To stay under $1 for the workshop, limit to short prompts and <100 queries.\n",
    "\n",
    "\n",
    "\n",
    "## Step 6: (Optional) Hugging Face local substitution\n",
    "\n",
    "To avoid managed API costs, you can install and run local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f38f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet sentence-transformers transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "local_embs = embedder.encode(corpus_df.text.tolist())\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct\")\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct\", device_map=\"auto\", load_in_4bit=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "\n",
    "def local_answer(query):\n",
    "    hits = retrieve(query).head(3)\n",
    "    context = \"\\n\\n\".join([f\"[{r.doc}] {r.text}\" for _, r in hits.iterrows()])\n",
    "    prompt = f\"Answer based only on this evidence:\\n{context}\\nQ: {query}\\nA:\"\n",
    "    return pipe(prompt, max_new_tokens=150)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c0502",
   "metadata": {},
   "source": [
    "**Trade-offs:**  \n",
    "- Hugging Face embeddings and generation are free but require powerful hardware.  \n",
    "- Vertex AI managed models cost less for small workloads and scale to large datasets automatically.  \n",
    "- No prebuilt “Hugging Face RAG” image exists on GCP; use `PyTorch` Workbench image and `pip install` as above.\n",
    "\n",
    "\n",
    "\n",
    "## Step 7: Cost summary\n",
    "\n",
    "| Step | Resource | Example Component | Cost Driver | Typical Range |\n",
    "|------|-----------|-------------------|--------------|----------------|\n",
    "| VM runtime | Vertex AI Workbench | `n1-standard-4` | Uptime (hourly) | ~$0.20/hr |\n",
    "| Embeddings | text-embedding-004 | Managed API | Tokens embedded | ~$0.10 / 1M tokens |\n",
    "| Retrieval | Local NN | CPU only | None | Free |\n",
    "| Generation | gemini-2.5-flash-001 | Managed API | Input/output tokens | ~$0.25 / 1M tokens |\n",
    "| Hugging Face alt | T4 VM | Local model inference | GPU uptime | ~$0.35/hr + egress |\n",
    "\n",
    "\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "- Use **Vertex AI managed embeddings** and **Gemini Flash** for lightweight, cost-controlled RAG.\n",
    "- Cache embeddings; reusing them saves most cost.\n",
    "- For open alternatives, use Hugging Face models on GPU VMs (higher cost, more control).\n",
    "- This workflow generalizes to any retrieval task — not just sustainability papers.\n",
    "- GCP’s managed tools lower barrier for experimentation while keeping enterprise security and IAM intact.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Vertex AI’s RAG stack = low-op, cost-predictable.  \n",
    "- Hugging Face = high control, high GPU cost.  \n",
    "- Keep data local or in GCS to manage egress and compliance.  \n",
    "- Always cite retrieved chunks for reproducibility and transparency.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
