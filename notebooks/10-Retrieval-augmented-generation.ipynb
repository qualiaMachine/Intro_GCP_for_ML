{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a24c78",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Retrieval-Augmented Generation (RAG) with Vertex AI\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How do we go from \"a pile of PDFs\" to \"ask a question and get a cited answer\" using Google Cloud tools?\n",
    "- What are the key parts of a RAG system (chunking, embedding, retrieval, generation), and how do they map onto Vertex AI services?\n",
    "- How much does each part of this pipeline cost (VM time, embeddings, LLM calls), and where can we keep it cheap?\n",
    "- Can we use open models / Hugging Face instead of Google models, and what does that change?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Unpack the core RAG pipeline: ingest → chunk → embed → retrieve → answer.\n",
    "- Run a minimal, fully programmatic RAG loop on a Vertex AI Workbench VM using Google’s own foundation models (for embeddings + generation).\n",
    "- Understand how to substitute open-source / Hugging Face models if you want to avoid managed API costs.\n",
    "- Answer questions using content from provided papers and return citations instead of vibes.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Overview: What we're building\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a pattern:\n",
    "\n",
    "1. You ask a question.  \n",
    "2. The system **retrieves** relevant passages from your PDFs or data.  \n",
    "3. An LLM **answers** using those passages only, with citations.\n",
    "\n",
    "This approach powers sustainability-related projects like **WattBot**, which extracts AI water and energy metrics from research papers.\n",
    "\n",
    "**Cost mindset:**  \n",
    "- **VM cost:** pay for Workbench instance uptime. Stop when not in use.  \n",
    "- **Embedding cost:** pay per character embedded — only once per doc.  \n",
    "- **Generation cost:** pay per token for input + output. Shorter prompts = cheaper.  \n",
    "\n",
    "**Hugging Face alternatives:**  \n",
    "You can replace Google-managed APIs with open models such as:  \n",
    "- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`, `BAAI/bge-large-en-v1.5`  \n",
    "- **Generators:** `google/gemma-2b-it`, `mistralai/Mistral-7B-Instruct`, or `tiiuae/falcon-7b-instruct`  \n",
    "However, this requires a GPU or large CPU VM (e.g., `n1-standard-8` + `T4`) and manual model management.  \n",
    "Vertex AI’s managed models (`text-embedding-004`, `gemini-2.5-flash-001`) are cost-optimized and scalable — better for workshops or low-ops setups.\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade google-cloud-aiplatform google-cloud-storage vertexai pypdf scikit-learn pandas\n",
    "!pip install --quiet --upgrade pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf11ba",
   "metadata": {},
   "source": [
    "**Cost note:** Installing packages is free; you're only billed for VM runtime.\n",
    "\n",
    "### Initialize project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from vertexai import init as vertexai_init\n",
    "import os\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"<YOUR_PROJECT_ID>\")\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "vertexai_init(project=PROJECT_ID, location=REGION)\n",
    "print(\"Initialized:\", PROJECT_ID, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f90ede",
   "metadata": {},
   "source": [
    "## Step 2: Extract and chunk PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, pathlib, re, pandas as pd\n",
    "from pypdf import PdfReader\n",
    "\n",
    "ZIP_PATH = pathlib.Path(\"/home/jupyter/Intro_GCP_for_ML/data/pdfs_bundle.zip\")\n",
    "DOC_DIR = pathlib.Path(\"/home/jupyter/docs\")\n",
    "DOC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# unzip\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    zf.extractall(DOC_DIR)\n",
    "\n",
    "def chunk_text(text, max_chars=1200, overlap=150):\n",
    "    for i in range(0, len(text), max_chars - overlap):\n",
    "        yield text[i:i+max_chars]\n",
    "\n",
    "rows = []\n",
    "for pdf in DOC_DIR.glob(\"*.pdf\"):\n",
    "    txt = \"\"\n",
    "    for page in PdfReader(str(pdf)).pages:\n",
    "        txt += page.extract_text() or \"\"\n",
    "    for i, chunk in enumerate(chunk_text(re.sub(r\"\\s+\", \" \", txt))):\n",
    "        rows.append({\"doc\": pdf.name, \"chunk_id\": i, \"text\": chunk})\n",
    "\n",
    "import pandas as pd\n",
    "corpus_df = pd.DataFrame(rows)\n",
    "print(len(corpus_df), \"chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbe505",
   "metadata": {},
   "source": [
    "**Cost note:** Only VM runtime applies. Chunk size affects future embedding cost.\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Embed text using Vertex AI\n",
    "\n",
    "### Choosing an embedding model\n",
    "\n",
    "Vertex AI currently offers multiple managed embedding models under the **Text Embeddings API** family.  \n",
    "For this exercise, we’re using **`text-embedding-004`**, which is Google’s latest general-purpose model optimized for **semantic similarity**, **retrieval**, and **clustering** tasks.  \n",
    "\n",
    "**Why this model?**\n",
    "- Produces 768-dimensional dense vectors suitable for cosine or dot-product similarity.  \n",
    "- Handles long passages (up to ~8,000 tokens) and multilingual content.  \n",
    "- Tuned for retrieval tasks like RAG, document search, and clustering.  \n",
    "- Cost-efficient for classroom-scale workloads (fractions of a cent per document).  \n",
    "\n",
    "If you’d like to explore other options:\n",
    "- Open the [**Vertex AI Model Garden → Text Embeddings**](https://console.cloud.google.com/vertex-ai/model-garden?project=doit-rci-mlm25-4626&pageState=(%22galleryStateKey%22:(%22f%22:(%22g%22:%5B%22goals%22%5D,%22o%22:%5B%22Text%20embeddings%22%5D),%22s%22:%22%22))) in your GCP console.  \n",
    "- You’ll find specialized alternatives such as:\n",
    "  - **`text-embedding-005` (experimental)** – larger model, higher precision on longer documents.  \n",
    "  - **`multimodal-embedding-001`** – supports image + text embeddings for richer use cases.  \n",
    "  - **Third-party embeddings (via Model Garden)** – e.g., `bge-large-en`, `cohere-embed-v3`, `all-MiniLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb46e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import numpy as np\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    vecs = []\n",
    "    for batch in [texts[i:i+32] for i in range(0, len(texts), 32)]:\n",
    "        resp = model.get_embeddings(batch)\n",
    "        for r in resp: vecs.append(r.values)\n",
    "    return np.array(vecs, dtype=\"float32\")\n",
    "\n",
    "emb_matrix = get_embeddings(corpus_df.text.tolist())\n",
    "print(\"Embeddings shape:\", emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86872d1e",
   "metadata": {},
   "source": [
    "**Cost note:** This is the first paid step — roughly $0.0001–0.0002 per 1k tokens.  \n",
    "Cache embeddings locally or in GCS to avoid recharging later.\n",
    "\n",
    "\n",
    "\n",
    "## Step 4: Retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96741dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(metric=\"cosine\", n_neighbors=5)\n",
    "nn.fit(emb_matrix)\n",
    "\n",
    "def retrieve(query):\n",
    "    q_vec = model.get_embeddings([query])[0].values\n",
    "    dist, idx = nn.kneighbors([q_vec], return_distance=True)\n",
    "    df = corpus_df.iloc[idx[0]].copy()\n",
    "    df[\"similarity\"] = 1 - dist[0]\n",
    "    return df.sort_values(\"similarity\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbfb32",
   "metadata": {},
   "source": [
    "**Cost note:** Retrieval is free (runs locally). Each new query triggers one embedding call.\n",
    "\n",
    "\n",
    "\n",
    "## Step 5: Generate answers using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b9d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "gmodel = GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def ask(query, top_k=5):\n",
    "    hits = retrieve(query).head(top_k)\n",
    "    context = \"\\n\\n\".join([f\"[{r.doc}#chunk-{r.chunk_id}] {r.text}\" for _, r in hits.iterrows()])\n",
    "    prompt = f\"You are a sustainability analyst. Use only the following context to answer the question.\\n\\n{context}\\n\\nQ: {query}\\nA:\"\n",
    "    ans = gmodel.generate_content(prompt)\n",
    "    return ans.text\n",
    "\n",
    "print(ask(\"What water usage (WUE) is reported for model training?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704dc6c6",
   "metadata": {},
   "source": [
    "**Cost note:** Gemini Flash costs ~$0.00005 per 1k input tokens and ~$0.0002 per 1k output tokens.  \n",
    "To stay under $1 for the workshop, limit to short prompts and <100 queries.\n",
    "\n",
    "\n",
    "## Step 6: Cost summary\n",
    "\n",
    "| Step | Resource | Example Component | Cost Driver | Typical Range |\n",
    "|------|-----------|-------------------|--------------|----------------|\n",
    "| VM runtime | Vertex AI Workbench | `n1-standard-4` | Uptime (hourly) | ~$0.20/hr |\n",
    "| Embeddings | text-embedding-004 | Managed API | Tokens embedded | ~$0.10 / 1M tokens |\n",
    "| Retrieval | Local NN | CPU only | None | Free |\n",
    "| Generation | gemini-2.5-flash-001 | Managed API | Input/output tokens | ~$0.25 / 1M tokens |\n",
    "| Hugging Face alt | T4 VM | Local model inference | GPU uptime | ~$0.35/hr + egress |\n",
    "\n",
    "\n",
    "## (Optional) Hugging Face local substitution\n",
    "\n",
    "To avoid managed API costs, you can instead using Hugging Face models. \n",
    "\n",
    "```python\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "- Use **Vertex AI managed embeddings** and **Gemini Flash** for lightweight, cost-controlled RAG.\n",
    "- Cache embeddings; reusing them saves most cost.\n",
    "- For open alternatives, use Hugging Face models on GPU VMs (higher cost, more control).\n",
    "- This workflow generalizes to any retrieval task — not just sustainability papers.\n",
    "- GCP’s managed tools lower barrier for experimentation while keeping enterprise security and IAM intact.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Vertex AI’s RAG stack = low-op, cost-predictable.  \n",
    "- Hugging Face = high control, high GPU cost.  \n",
    "- Keep data local or in GCS to manage egress and compliance.  \n",
    "- Always cite retrieved chunks for reproducibility and transparency.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
