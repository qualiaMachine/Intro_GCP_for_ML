{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231a3002",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Retrieval-Augmented Generation (RAG) with Vertex AI\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How do we go from \"a pile of PDFs\" to \"ask a question and get a cited answer\" using Google Cloud tools?\n",
    "- What are the key parts of a RAG system (chunking, embedding, retrieval, generation), and how do they map onto Vertex AI services?\n",
    "- How much does each part of this pipeline cost (VM time, embeddings, LLM calls), and where can we keep it cheap?\n",
    "- Can we use open models / Hugging Face instead of Google models, and what does that change?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Unpack the core RAG pipeline: ingest → chunk → embed → retrieve → answer.\n",
    "- Run a minimal, fully programmatic RAG loop on a Vertex AI Workbench VM using Google’s own foundation models (for embeddings + generation).\n",
    "- Understand how to substitute open-source / Hugging Face models if you want to avoid managed API costs.\n",
    "- Answer questions using content from provided papers and return citations instead of vibes.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## Overview: What we're building\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a pattern:\n",
    "\n",
    "1. You ask a question.  \n",
    "2. The system **retrieves** relevant passages from your PDFs or data.  \n",
    "3. An LLM **answers** using those passages only, with citations.\n",
    "\n",
    "This approach powers sustainability-related projects like **WattBot**, which extracts AI water and energy metrics from research papers.\n",
    "\n",
    "**Cost mindset:**  \n",
    "- **VM cost:** pay for Workbench instance uptime. Stop when not in use.  \n",
    "- **Embedding cost:** pay per character embedded — only once per doc.  \n",
    "- **Generation cost:** pay per token for input + output. Shorter prompts = cheaper.  \n",
    "\n",
    "**Hugging Face alternatives:**  \n",
    "You can replace Google-managed APIs with open models such as:  \n",
    "- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`, `BAAI/bge-large-en-v1.5`  \n",
    "- **Generators:** `google/gemma-2b-it`, `mistralai/Mistral-7B-Instruct`, or `tiiuae/falcon-7b-instruct`  \n",
    "However, this requires a GPU or large CPU VM (e.g., `n1-standard-8` + `T4`) and manual model management. Rather than use a very expensive machine and GPU in Workbench, you can launch custom jobs that perform the embedding and generation steps. Start with a PyTorch image and add HuggingFace as a requirement.\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8520c5",
   "metadata": {},
   "source": [
    "**Cost note:** Installing packages is free; you're only billed for VM runtime.\n",
    "\n",
    "### Initialize project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6302336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from vertexai import init as vertexai_init\n",
    "import os\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"<YOUR_PROJECT_ID>\")\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "vertexai_init(project=PROJECT_ID, location=REGION)\n",
    "print(\"Initialized:\", PROJECT_ID, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ef210",
   "metadata": {},
   "source": [
    "## Step 2: Extract and chunk PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5762af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, pathlib, re, pandas as pd\n",
    "from pypdf import PdfReader\n",
    "\n",
    "ZIP_PATH = pathlib.Path(\"/home/jupyter/Intro_GCP_for_ML/data/pdfs_bundle.zip\")\n",
    "DOC_DIR = pathlib.Path(\"/home/jupyter/docs\")\n",
    "DOC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# unzip\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    zf.extractall(DOC_DIR)\n",
    "\n",
    "def chunk_text(text, max_chars=1200, overlap=150):\n",
    "    for i in range(0, len(text), max_chars - overlap):\n",
    "        yield text[i:i+max_chars]\n",
    "\n",
    "rows = []\n",
    "for pdf in DOC_DIR.glob(\"*.pdf\"):\n",
    "    txt = \"\"\n",
    "    for page in PdfReader(str(pdf)).pages:\n",
    "        txt += page.extract_text() or \"\"\n",
    "    for i, chunk in enumerate(chunk_text(re.sub(r\"\\s+\", \" \", txt))):\n",
    "        rows.append({\"doc\": pdf.name, \"chunk_id\": i, \"text\": chunk})\n",
    "\n",
    "import pandas as pd\n",
    "corpus_df = pd.DataFrame(rows)\n",
    "print(len(corpus_df), \"chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ea09d",
   "metadata": {},
   "source": [
    "**Cost note:** Only VM runtime applies. Chunk size affects future embedding cost.\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Embed text using Vertex AI\n",
    "\n",
    "### Choosing an embedding and generator model\n",
    "\n",
    "Vertex AI currently offers multiple managed embedding models under the **Text Embeddings API** family.  \n",
    "For this exercise, we’re using **`text-embedding-004`**, which is Google’s latest general-purpose model optimized for **semantic similarity**, **retrieval**, and **clustering** tasks.  \n",
    "\n",
    "**Why this model?**\n",
    "- Produces 768-dimensional dense vectors suitable for cosine or dot-product similarity.  \n",
    "- Handles long passages (up to ~8,000 tokens) and multilingual content.  \n",
    "- Tuned for retrieval tasks like RAG, document search, and clustering.  \n",
    "- Cost-efficient for classroom-scale workloads (fractions of a cent per document).  \n",
    "\n",
    "If you’d like to explore other options:\n",
    "- Open the [**Vertex AI Model Garden → Text Embeddings**](https://console.cloud.google.com/vertex-ai/model-garden?project=doit-rci-mlm25-4626&pageState=(%22galleryStateKey%22:(%22f%22:(%22g%22:%5B%22goals%22%5D,%22o%22:%5B%22Text%20embeddings%22%5D),%22s%22:%22%22))) in your GCP console.  \n",
    "- You’ll find specialized alternatives such as:\n",
    "  - **`text-embedding-005` (experimental)** – larger model, higher precision on longer documents.  \n",
    "  - **`multimodal-embedding-001`** – supports image + text embeddings for richer use cases.  \n",
    "  - **Third-party embeddings (via Model Garden)** – e.g., `bge-large-en`, `cohere-embed-v3`, `all-MiniLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01727891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1. Imports and client setup\n",
    "#############################################\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions, EmbedContentConfig, GenerateContentConfig\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# We'll assume you already have:\n",
    "#   corpus_df  -> pandas DataFrame with columns: 'text', 'doc', 'chunk_id'\n",
    "# If not, you'll need to define/load that before running this cell.\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 2. Initialize the Gen AI client\n",
    "#############################################\n",
    "\n",
    "# vertexai=True = bill/govern in your GCP project instead of the public endpoint\n",
    "client = genai.Client(\n",
    "    http_options=HttpOptions(api_version=\"v1\"),\n",
    "    vertexai=True,\n",
    "    project=\"doit-rci-mlm25-4626\",\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# Generation model for answering questions\n",
    "GENERATION_MODEL_ID = \"gemini-2.5-pro\"        # or \"gemini-2.5-flash\" for cheaper/faster\n",
    "\n",
    "# Embedding model for retrieval\n",
    "EMBED_MODEL_ID = \"gemini-embedding-001\"\n",
    "\n",
    "# Pick an embedding dimensionality and stick to it across corpus + queries.\n",
    "EMBED_DIM = 1536  # valid typical choices: 768, 1536, 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 3. Helper: get embeddings for a list of texts\n",
    "#############################################\n",
    "\n",
    "def embed_texts(text_list, batch_size=32, dims=EMBED_DIM):\n",
    "    \"\"\"\n",
    "    Convert a list of text strings into embedding vectors using gemini-embedding-001.\n",
    "    Returns a NumPy array of shape (len(text_list), dims).\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "\n",
    "    # batch to avoid huge single requests\n",
    "    for start in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[start:start+batch_size]\n",
    "\n",
    "        resp = client.models.embed_content(\n",
    "            model=EMBED_MODEL_ID,\n",
    "            contents=batch,\n",
    "            config=EmbedContentConfig(\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\",   # optimize embeddings for retrieval/use as chunks\n",
    "                output_dimensionality=dims,       # must match EMBED_DIM everywhere\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # resp.embeddings is aligned with 'batch'\n",
    "        for emb in resp.embeddings:\n",
    "            vectors.append(emb.values)\n",
    "\n",
    "    return np.array(vectors, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8281f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 4. Embed the corpus and build the NN index\n",
    "#############################################\n",
    "\n",
    "# Create embeddings for every text chunk in the corpus\n",
    "emb_matrix = embed_texts(corpus_df[\"text\"].tolist(), dims=EMBED_DIM)\n",
    "print(\"emb_matrix shape:\", emb_matrix.shape)   # (num_chunks, EMBED_DIM)\n",
    "\n",
    "# Fit NearestNeighbors on those embeddings once\n",
    "nn = NearestNeighbors(\n",
    "    metric=\"cosine\",   # cosine distance is standard for semantic similarity\n",
    "    n_neighbors=5,     # default neighborhood size; can override at query time\n",
    ")\n",
    "nn.fit(emb_matrix)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 5. Retrieval: given a query string, get top-k relevant chunks\n",
    "#############################################\n",
    "\n",
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Embed the user query with the SAME embedding model/dim,\n",
    "    then find the top-k most similar corpus chunks.\n",
    "    Returns a DataFrame of the top matches with a 'similarity' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query to the same dimension space as emb_matrix\n",
    "    query_vec = embed_texts([query], dims=EMBED_DIM)[0]   # shape (EMBED_DIM,)\n",
    "\n",
    "    # Find nearest neighbors using cosine distance\n",
    "    distances, indices = nn.kneighbors([query_vec], n_neighbors=k, return_distance=True)\n",
    "\n",
    "    # Grab those rows from the original corpus\n",
    "    result_df = corpus_df.iloc[indices[0]].copy()\n",
    "\n",
    "    # Convert cosine distance -> cosine similarity (1 - distance)\n",
    "    result_df[\"similarity\"] = 1 - distances[0]\n",
    "\n",
    "    # Sort by similarity descending (highest similarity first)\n",
    "    result_df = result_df.sort_values(\"similarity\", ascending=False)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4df150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 6. ask(): build grounded prompt + call Gemini to answer\n",
    "#############################################\n",
    "\n",
    "def ask(query, top_k=5, temperature=0.2):\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation:\n",
    "    - retrieve context chunks relevant to `query`\n",
    "    - stuff those chunks into a prompt\n",
    "    - ask Gemini to answer ONLY using that context\n",
    "    \"\"\"\n",
    "\n",
    "    # Get top_k most relevant text chunks\n",
    "    hits = retrieve(query, k=top_k)\n",
    "\n",
    "    # Build a context block with provenance tags like [doc#chunk-id]\n",
    "    context_lines = [\n",
    "        f\"[{row.doc}#chunk-{row.chunk_id}] {row.text}\"\n",
    "        for _, row in hits.iterrows()\n",
    "    ]\n",
    "    context_block = \"\\n\\n\".join(context_lines)\n",
    "\n",
    "    # Instruction prompt for the model\n",
    "    prompt = (\n",
    "        \"You are a sustainability analyst. \"\n",
    "        \"Use only the following context to answer the question.\\n\\n\"\n",
    "        f\"{context_block}\\n\\n\"\n",
    "        f\"Q: {query}\\n\"\n",
    "        \"A:\"\n",
    "    )\n",
    "\n",
    "    # Call the generative model\n",
    "    response = client.models.generate_content(\n",
    "        model=GENERATION_MODEL_ID,\n",
    "        contents=prompt,\n",
    "        config=GenerateContentConfig(\n",
    "            temperature=temperature,  # lower = more deterministic, factual\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Return the model's answer text\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc1772",
   "metadata": {},
   "source": [
    "## Step 5: Generate answers using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8446990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 7. Test the pipeline end-to-end\n",
    "#############################################\n",
    "\n",
    "print(\n",
    "    ask(\n",
    "        \"What is the name of the benchmark suite presented in a recent paper \"\n",
    "        \"for measuring inference energy consumption?\"\n",
    "    )\n",
    ")\n",
    "# Expected answer: \"ML.ENERGY Benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47146b6f",
   "metadata": {},
   "source": [
    "## Step 6: Cost summary\n",
    "\n",
    "| Step | Resource | Example Component | Cost Driver | Typical Range |\n",
    "|------|-----------|-------------------|--------------|----------------|\n",
    "| VM runtime | Vertex AI Workbench | `n1-standard-4` | Uptime (hourly) | ~$0.20/hr |\n",
    "| Embeddings | text-embedding-004 | Managed API | Tokens embedded | ~$0.10 / 1M tokens |\n",
    "| Retrieval | Local NN | CPU only | None | Free |\n",
    "| Generation | gemini-2.5-flash-001 | Managed API | Input/output tokens | ~$0.25 / 1M tokens |\n",
    "| Hugging Face alt | T4 VM | Local model inference | GPU uptime | ~$0.35/hr + egress |\n",
    "\n",
    "\n",
    "## (Optional) Hugging Face local substitution\n",
    "\n",
    "To avoid managed API costs, you can instead using Hugging Face models. \n",
    "\n",
    "```python\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "- Use **Vertex AI managed embeddings** and **Gemini Flash** for lightweight, cost-controlled RAG.\n",
    "- Cache embeddings; reusing them saves most cost.\n",
    "- For open alternatives, use Hugging Face models on GPU VMs (higher cost, more control).\n",
    "- This workflow generalizes to any retrieval task — not just sustainability papers.\n",
    "- GCP’s managed tools lower barrier for experimentation while keeping enterprise security and IAM intact.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Vertex AI’s RAG stack = low-op, cost-predictable.  \n",
    "- Hugging Face = high control, high GPU cost.  \n",
    "- Keep data local or in GCS to manage egress and compliance.  \n",
    "- Always cite retrieved chunks for reproducibility and transparency.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
